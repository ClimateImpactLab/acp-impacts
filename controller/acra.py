# -*- coding: utf-8 -*-
"""ACRA results and diagnostics.

The main function for generating result bundles is make_results().

Functions in the "Result Generation Request Functions" set are used by
'paster request' to call make_results with different arguments.
"""

## Imports

import math, os, csv, random, tarfile, StringIO, re
import numpy as np
try:
    from netCDF4 import Dataset # Required for some functions
except:
    pass

from openest.dmas import remote, server
from ..impacts import agriculture, fake, daily
from ..extract import results, acptable, weightstable, unweightedtable
from ..census import census
from ..crime import crime
from ..mortality import mortality
from ..adaptation.adapting_curve import AdaptingCurve, SimpleAdaptingCurve
from ..iam import effect_bundle, counties, weather, fake_weather
from ..regional import aggregations
from openest.models.memoizable import MemoizedUnivariate
from openest.models.curve import FlatCurve, StepCurve, CurveCurve
from openest.models.spline_model import SplineModel
from openest.models.bin_model import BinModel
from openest.models.univariate_model import UnivariateModel
import merging

__all__ = ['ACRAController'] # Used by TG2

# These files in each bundle directory flag that the files have been checked
checked_file = 'check-20140609'
checked_file_cge = 'cgechk-20140609' # Don't need crime or energy checked for CGE

# Convert from Fahrenheit to celsius
def f2c(f):
    return (f - 32) / 1.8

# These should be called from a paster request on the server (web will timeout)
class ACRAController(object):
    # Note special handling for crime adaptation and comparisons to
    # it, since in this case, we use an unmerged baseline for crime,
    # and replace the 65 degree regional bin with it.

    # All of the models used for various calculations
    #   name format: <output impact>_<weather input>_<model|url>
    #    -- model defines a MongoDB ID
    #    -- url defines a URL that produces the model results (e.g., after merging)
    #   URLs are for merging "retrieval", as generated by the commented-out URL
    models = dict(
        wheat_tas_model = '52e053a6434fd7183d7530ca',
        wheat_co2_model = '52fd63a0434fd7239d3eaf37',
        maize_east_tas_model = '52d9b75b434fd7155b1829cb',
        maize_west_tas_model = '52d9b7a7434fd71563b43b2b',
        maize_east_pr_model = '52f1c630434fd74939ea077d',
        maize_west_pr_model = '52f1c8cf434fd72cd6821d15',
        maize_co2_model = '52fd63c8434fd723a13eaf33',
        cotton_tas_model = '52d9b927434fd7155a1829d8',
        cotton_pr_model = '52f1cbe2434fd72cd7821d27',
        cotton_co2_url = server.domain_url("/collection/retrieve_hierarchical_normal_merge_muid?collection_id=52fd5881434fd7239d3eaf33&simuids=53569b37434fd73a8a0fdb04,53593c79434fd732b19d6498&ddp_rescale=1"), #server.domain_url("/collection/generate_hierarchical_normal_merge_muid?collection_id=52fd5881434fd7239d3eaf33&meta_ids=52fd63a0434fd7239d3eaf37%2C52fd63c8434fd723a13eaf33%2C52fd6452434fd723a03eaf3a%2C52fd646c434fd728bfc104aa%2C52fd6485434fd7239d3eaf40%2C52fd64c5434fd728c7c1049f%2C52fd64e1434fd7239f3eaf41%2C52fd64fb434fd728bfc104b4%2C52fd6513434fd728bfc104bd&weights=1%2C1%2C1%2C1%2C1%2C1%2C1%2C1%2C1&force_doseless=1&ddp_rescale=1&iterations=100000"),
        soy_east_tas_model = '52d9b88d434fd714c8186f8a',
        soy_west_tas_model = '52d9b89f434fd7155fb43b34',
        soy_east_pr_model = '52f1c996434fd749347ff0d2',
        soy_west_pr_model = '52f1cb51434fd72cd9821d1c',
        soy_co2_model = '52fd6485434fd7239d3eaf40',
        mortality_tas_url = server.domain_url("/collection/retrieve_hierarchical_normal_merge_muid?collection_id=525dded88309fa1cbb844c96&simuids=53594152434fd733f817dd29,53594152434fd733f817dd2b,53594152434fd733f817dd2d,53594152434fd733f817dd2f,53594153434fd733f817dd31,53594153434fd733f817dd33,53594153434fd733f817dd35,53594153434fd733f817dd37,53594153434fd733f817dd39,53594153434fd733f817dd3b,53594152434fd733f817dd2a,53594152434fd733f817dd2c,53594152434fd733f817dd2e,53594153434fd733f817dd30,53594153434fd733f817dd32,53594153434fd733f817dd34,53594153434fd733f817dd36,53594153434fd733f817dd38,53594153434fd733f817dd3a,53594153434fd733f817dd3c"), #server.domain_url("/collection/generate_hierarchical_normal_merge_muid?collection_id=525dded88309fa1cbb844c96&meta_ids=5279840c8309fa1a04a6ada1%2C531f48ea434fd70e335f3c75&weights=1%2C1&iterations=100000"),
        mortality_0_0_tas_model = '5331255d434fd7296e094be4',
        mortality_1_44_tas_model = '53312575434fd76f85ca5ba1',
        mortality_45_64_tas_model = '53312597434fd7297fd70093',
        mortality_65_inf_tas_model = '533125ae434fd76f81ca5bc0',
        labor_high_tasmax_model = '52b248af434fd772cf74096f',
        labor_low_tasmax_model = '531e239c434fd70d39b2bc7d',
        crime_violent_adaptable_tasmax_url = server.domain_url("/collection/merge_collection?id=5367e0bd434fd70f307a8335"),
        crime_violent_tasmax_url = server.domain_url("/collection/retrieve_hierarchical_normal_merge_muid?collection_id=52d9aade434fd77f09ec4f62&simuids=53594263434fd733f817dd3d,53594263434fd733f817dd3f,53594263434fd733f817dd41,53594263434fd733f817dd43,53594263434fd733f817dd45,53594263434fd733f817dd47,53594263434fd733f817dd49,53594263434fd733f817dd4b,53594263434fd733f817dd4d,53594263434fd733f817dd4f,53594264434fd733f817dd51,53594263434fd733f817dd3e,53594263434fd733f817dd40,53594263434fd733f817dd42,53594263434fd733f817dd44,53594263434fd733f817dd46,53594263434fd733f817dd48,53594263434fd733f817dd4a,53594263434fd733f817dd4c,53594263434fd733f817dd4e,53594264434fd733f817dd50,53594264434fd733f817dd52"), #server.domain_url("/collection/generate_hierarchical_normal_merge_muid?collection_id=52d9aade434fd77f09ec4f62&meta_ids=53226d8c434fd71dd08516f1%2C53226dca434fd71dc88516f1%2C53226e37434fd71dcf8516f7%2C53226e46434fd71dc54f9114%2C53227d4c434fd71dca8516fa&weights=1%2C1%2C1%2C1%2C1&iterations=100000"),
        crime_violent_pr_url = server.domain_url("/collection/retrieve_hierarchical_normal_merge_muid?collection_id=53228e61434fd72e07279471&simuids=53594eb9434fd733f817dd53,53594eb9434fd733f817dd55,53594eb9434fd733f817dd57,53594eb9434fd733f817dd59,53594eb9434fd733f817dd5b,53594eb9434fd733f817dd54,53594eb9434fd733f817dd56,53594eb9434fd733f817dd58,53594eb9434fd733f817dd5a,53594eb9434fd733f817dd5c"), # server.domain_url("/collection/generate_hierarchical_normal_merge_muid?collection_id=53228e61434fd72e07279471&meta_ids=53228e6b434fd71dca851713%2C53228e96434fd71dc685172d%2C53228eaf434fd72df8cbb08d%2C53228ee5434fd72df9cbb072%2C53228f09434fd71dd085172c&weights=1%2C1%2C1%2C1%2C1&iterations=100000"),
        crime_property_adaptable_tasmax_url = server.domain_url("/collection/merge_collection?id=5367e26c434fd71007fde2d1"),
        crime_property_tasmax_url = server.domain_url("/collection/retrieve_hierarchical_normal_merge_muid?collection_id=52d9ac86434fd7120546c3aa&simuids=53594fd2434fd733f817dd5d,53594fd2434fd733f817dd5f,53594fd2434fd733f817dd61,53594fd2434fd733f817dd63,53594fd2434fd733f817dd65,53594fd2434fd733f817dd67,53594fd2434fd733f817dd69,53594fd2434fd733f817dd6b,53594fd2434fd733f817dd6d,53594fd3434fd733f817dd6f,53594fd3434fd733f817dd71,53594fd2434fd733f817dd5e,53594fd2434fd733f817dd60,53594fd2434fd733f817dd62,53594fd2434fd733f817dd64,53594fd2434fd733f817dd66,53594fd2434fd733f817dd68,53594fd2434fd733f817dd6a,53594fd2434fd733f817dd6c,53594fd3434fd733f817dd6e,53594fd3434fd733f817dd70,53594fd3434fd733f817dd72"), #server.domain_url("/collection/generate_hierarchical_normal_merge_muid?collection_id=52d9ac86434fd7120546c3aa&meta_ids=53227dd8434fd77988cee23b%2C53227dfd434fd71dc885170d%2C53227e19434fd7798acee23b%2C53227e37434fd77988cee244%2C53228425434fd71dcf851712&weights=1%2C1%2C1%2C1%2C1&iterations=100000"),
        crime_property_pr_url = server.domain_url("/collection/retrieve_hierarchical_normal_merge_muid?collection_id=53228ad6434fd72e0727945e&simuids=53595080434fd733f817dd73,53595080434fd733f817dd75,53595080434fd733f817dd77,53595080434fd733f817dd79,53595080434fd733f817dd7b,53595080434fd733f817dd74,53595080434fd733f817dd76,53595080434fd733f817dd78,53595080434fd733f817dd7a,53595080434fd733f817dd7c"), #server.domain_url("/collection/generate_hierarchical_normal_merge_muid?collection_id=53228ad6434fd72e0727945e&meta_ids=53228aed434fd72dfacbb071%2C53228b12434fd72dfccbb071%2C53228b42434fd71dd0851717%2C53228b72434fd72e07279464%2C53228f49434fd71dc685173c&weights=1%2C1%2C1%2C1%2C1&iterations=100000"),
        energy_tas_model = '527173c38309fa036fab4405')

    # Information for adaptation processing
    #   dictionaries used by the adaptation generators and make_adapting_curve()
    adaptation = dict(
        maize_east = {
            'baseline': 'maize_east_tas_model',
            'future': 'maize_west_tas_model', # Have a unidirectional approach to 'future' model
            #'time': [(1960, SplineModel.create_gaussian({30: (-.0060897, .0016109*.0016109)})),
            #         (1995, SplineModel.create_gaussian({30: (-.0034439, .0016444*.0016444)}))],
            'gamma': SplineModel.create_gaussian({0: (.99722, .00573*.00573)}), # A normal approximation to MC gamma
            'cut-point': 'killdd', # Only apply to killing degree-days
            'beta-infinity': 0 # Approach no killing degree-days
        },
        maize_east_gddkdd = {
            'baseline': 'maize_east_tas_model',
            'future': 'maize_west_tas_model', # Have a unidirectional approach to 'future' model
            #'time': [(1960, SplineModel.create_gaussian({30: (-.0060897, .0016109*.0016109)})),
            #         (1995, SplineModel.create_gaussian({30: (-.0034439, .0016444*.0016444)}))],
            'gamma': SplineModel.create_gaussian({0: (.99722, .00573*.00573)}), # A normal approximation to MC gamma
            'cut-point': 'bothdd'
        },
        crime_violent = {
            'baseline': 'crime_violent_adaptable_tasmax_url',
            'baseline_pval': 'crime_violent_tasmax_url',
            'space': {f2c(45): server.domain_url("/collection/merge_collection?id=5367da3b434fd709a3cbe6c8"),
                      f2c(55): server.domain_url("/collection/merge_collection?id=5367dfca434fd70ef45eb0b7"),
                      # Uncomment out the next line to be able to plot it with make_crime
                      #f2c(65): server.domain_url("/collection/merge_collection?id=5367e0bd434fd70f307a8335"),
                      f2c(75): server.domain_url("/collection/merge_collection?id=5367e170434fd70f2c7a8351")},
            'time': [(1960, server.domain_url("/collection/merge_collection?id=5367e604434fd70fc23a00eb")),
                     (2000, server.domain_url("/collection/merge_collection?id=5367e6d2434fd70fc03a00ec"))],
            'Wbar-baseline': f2c(65), # From Ranson's data # For plotting, use f2c(65.88603)
            'cut-point': f2c(65), # Use two adaptation curves
            'beta-infinity': 0, # Approach no temperature-effect on crime
            'clip-zero': False # Allow negative temperature bin effect
        },
        crime_property = {
            'baseline': 'crime_property_adaptable_tasmax_url',
            'baseline_pval': 'crime_property_tasmax_url',
            'space': {f2c(45): server.domain_url("/collection/merge_collection?id=5367e255434fd71005fde2d1"),
                      f2c(55): server.domain_url("/collection/merge_collection?id=5367e261434fd70fbf3a0098"),
                      # Uncomment out the next line to be able to plot it with make_crime
                      #f2c(65): server.domain_url("/collection/merge_collection?id=5367e26c434fd71007fde2d1"),
                      f2c(75): server.domain_url("/collection/merge_collection?id=5367e279434fd70fbe3a007d")},
            'time': [(1960, server.domain_url("/collection/merge_collection?id=5367e807434fd70fc23a0111")),
                     (2000, server.domain_url("/collection/merge_collection?id=5367e767434fd70fc23a00fd"))],
            'Wbar-baseline': f2c(65), # From Ranson's data # For plotting, use f2c(65.88603)
            'cut-point': f2c(65), # Use two adaptation curves
            'beta-infinity': 0, # Approach no temperature-effect on crime
            'clip-zero': False # Allow negative temperature bin effect
        },
        mortality = {
            'baseline': 'mortality_tas_url',
            'space': {f2c((40*112.6 + 60*(365 - (112.6 + 7.9 + 0.05)) + 85*7.9 + 90*0.05) / 365): '53680c9b434fd70fc23a013c',
                      f2c((40*120.6 + 60*(365 - (120.6 + 11.0 + 0.15)) + 85*11.0 + 90*0.15) / 365): '53680d71434fd70fbf3a0155',
                      f2c((40*37.7 + 60*(365 - (37.7 + 54.8 + 0.78)) + 85*54.8 + 90*0.78) / 365): '53680d7a434fd70fc13a0122',
                      f2c((40*35.2 + 60*(365 - (35.2 + 15.5 + 4.27)) + 85*15.5 + 90*4.27) / 365): '53680d65434fd70fbf3a014c'},
            'time': [(1945, '536805a8434fd70fbf3a0133'),
                     (1982, '531f48ea434fd70e335f3c75')],
            'Wbar-baseline': f2c((40*73.7 + 60*(365 - (73.7 + 26.0 + 1.14)) + 85*26.0 + 90*1.14) / 365), # Baseline temperature from paper
            'cut-point': f2c(65), # Use two adaptation curves
            'beta-infinity': 0, # Approach no temperature-effect on mortality
            'clip-zero': True # Do not allow negative temperature bin effects
        })

    ### General Status Functions

    def count_results(self):
        """Produce a table of the number of valid results in each RCP and batch."""

        rcps = ["rcp26", "rcp45", "rcp60", "rcp85"]
        print "\t".join(["batch"] + rcps + rcps) # header row

        # Iterate through MC batches
        for batchnum in range(25):
            counts = {rcp: 0 for rcp in rcps} # how many valid results per RCP
            counts_cge = {rcp: 0 for rcp in rcps} # how many valid results for the CGE per rcp

            batch = 'batch-' + str(batchnum)

            # Iterate through all subdirectories
            for rcp in os.listdir(batch):
                for model in os.listdir(os.path.join(batch, rcp)):
                    for realization in os.listdir(os.path.join(batch, rcp, model)):

                        targetdir = os.path.join(batch, rcp, model, realization)

                        # Check if flag files are in directories
                        files = os.listdir(targetdir)
                        if checked_file in files:
                            counts[rcp] = counts[rcp] + 1
                        if checked_file_cge in files:
                            counts_cge[rcp] = counts_cge[rcp] + 1

            print "\t".join([batch] + [str(counts[rcp]) for rcp in rcps] + [str(counts_cge[rcp]) for rcp in rcps])

        # Iterate through constant p-value directories
        pdirs = ['pmed', 'plow', 'phigh']
        for pdir in pdirs:
            counts = {rcp: 0 for rcp in rcps} # how many valid results per RCP
            counts_cge = {rcp: 0 for rcp in rcps} # how many valid results for the CGE per rcp

            if not os.path.exists(pdir):
                continue

            # Iterate through all subdirectories
            for rcp in os.listdir(pdir):
                for model in os.listdir(os.path.join(pdir, rcp)):
                    for realization in os.listdir(os.path.join(pdir, rcp, model)):

                        targetdir = os.path.join(pdir, rcp, model, realization)

                        # Check if flag files are in directories
                        files = os.listdir(targetdir)
                        if checked_file in files:
                            counts[rcp] = counts[rcp] + 1
                        if checked_file_cge in files:
                            counts_cge[rcp] = counts_cge[rcp] + 1

            print "\t".join([pdir] + [str(counts[rcp]) for rcp in rcps] + [str(counts_cge[rcp]) for rcp in rcps])

    ### Additional Results and Operations

    def generate_labor_total(self):
        """Combine low-risk and high-risk results into a combined labor result."""

        # Iterate through all MC directories
        for batchnum in range(25):
            batch = 'batch-' + str(batchnum)

            for rcp in os.listdir(batch):
                for model in os.listdir(os.path.join(batch, rcp)):
                    for realization in os.listdir(os.path.join(batch, rcp, model)):

                        targetdir = os.path.join(batch, rcp, model, realization)
                        print targetdir

                        # Produce the labor total
                        self.make_labor_total(targetdir)

    def regional_aggregation(self):
        """Aggregate counties according to midwest metropolitan regions."""

        #regions = aggregations.load_region_definitions(aggregator.__path__[0] + "/lib/acra/regional/midwest_regions.csv", 0, 1) # Midwest
        regions = aggregations.load_region_definitions(aggregator.__path__[0] + "/lib/acra/regional/california_regions.csv", 0, 2) # California
        get_region = lambda fips: regions.get(fips, None) # Construct a aggregation function to pass to aggregate_tar()

        working = os.getcwd()

        # Iterate through all result directories
        for batch in (['batch-' + str(n) for n in range(25)] + ['pmed', 'plow', 'phigh']):
            for rcp in os.listdir(batch):
                for model in os.listdir(os.path.join(batch, rcp)):
                    for realization in os.listdir(os.path.join(batch, rcp, model)):

                        targetdir = os.path.join(batch, rcp, model, realization)
                        print targetdir

                        # Loop through all possible results, and reaggregate
                        try:
                            # Agriculture:
                            # Do both w/ and w/o CO2
                            for suffix in ['', '-noco2']:
                                if 'yields-maize' + suffix + '.tar.gz' in os.listdir(targetdir):
                                    agriculture.aggregate_tar_with_scale_file('yields-maize' + suffix, ['maize-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=get_region)

                                if 'yields-wheat' + suffix + '.tar.gz' in os.listdir(targetdir):
                                    agriculture.aggregate_tar_with_scale_file('yields-wheat' + suffix, ['wheat-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=get_region)

                                if 'yields-grains' + suffix + '.tar.gz' in os.listdir(targetdir):
                                    agriculture.aggregate_tar_with_scale_file('yields-grains' + suffix, ['maize-planted','wheat-planted'], [1690.,1615.], targetdir, collabel=['relative', 'output', 'production'], get_region=get_region) # aggregate grains by calories

                                if 'yields-cotton' + suffix + '.tar.gz' in os.listdir(targetdir):
                                    agriculture.aggregate_tar_with_scale_file('yields-cotton' + suffix, ['cotton-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=get_region)

                                if 'yields-oilcrop' + suffix + '.tar.gz' in os.listdir(targetdir):
                                    agriculture.aggregate_tar_with_scale_file('yields-oilcrop' + suffix, ['soy-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=get_region)

                                if 'yields-total' + suffix + '.tar.gz' in os.listdir(targetdir):
                                    agriculture.aggregate_tar_with_scale_file('yields-total' + suffix, ['wheat-planted', 'maize-planted', 'cotton-planted', 'soy-planted'], [1, 1, 1, 1], targetdir, collabel=['relative', 'output', 'production'], get_region=get_region) # aggregate all by MT

                            # Crime:
                            if 'crime-violent.tar.gz' in os.listdir(targetdir):
                                ACRAController.crime_aggregate_tar('crime-violent', targetdir, collabel=['relative', 'impact'], get_region=get_region)

                            if 'crime-property.tar.gz' in os.listdir(targetdir):
                                ACRAController.crime_aggregate_tar('crime-property', targetdir, collabel=['relative', 'impact'], get_region=get_region)

                            # Energy:
                            if 'energy-residential.tar.gz' in os.listdir(targetdir):
                                ACRAController.population_aggregate_tar('energy-residential', targetdir, get_region=get_region)

                            # Health:
                            if 'health-mortality.tar.gz' in os.listdir(targetdir):
                                ACRAController.population_aggregate_tar('health-mortality', targetdir, collabel=["addlrate", 'output'], get_region=get_region)

                            for bounds in ["0-0", "1-44", "45-64", "65-inf"]:
                                if 'health-mortage-' + bounds + '.tar.gz' in os.listdir(targetdir):
                                    ACRAController.population_aggregate_tar('health-mortage-' + bounds, targetdir, collabel=["addlrate", 'output'], get_region=get_region)

                            # Labor:
                            if 'labor-high-productivity.tar.gz' in os.listdir(targetdir):
                                ACRAController.labor_aggregate_tar('labor-high-productivity', targetdir, True, collabel=['fraction', 'output'], get_region=get_region)

                            if 'labor-low-productivity.tar.gz' in os.listdir(targetdir):
                                ACRAController.labor_aggregate_tar('labor-low-productivity', targetdir, False, collabel=['fraction', 'output'], get_region=get_region)

                            if 'labor-total-productivity.tar.gz' in os.listdir(targetdir):
                                ACRAController.labor_total_aggregate_tar('labor-total-productivity', targetdir, collabel=['fraction', 'output'], get_region=get_region)
                        except:
                            os.chdir(working) # return to previous directory (would happen automatically if completed)
                            print "ERROR"

    def fix_truehist_years(self):
        """In first attempt to generate historical results, years were misnumbered.  Renumber them."""

        # Iterate through all result directories
        for (batch, rcp, model, realization, pvals, targetdir) in results.iterate_byp('/home/jrising/impacts2', 'truehist'): #results.iterate_montecarlo("/home/jrising/impacts2", "truehist"):
            for filename in os.listdir(targetdir):
                # Only operate on result bundles
                if filename[-6:] != 'tar.gz':
                    continue

                # Check if this result tar needs to be fixed
                with tarfile.open(os.path.join(targetdir, filename)) as tar:
                    year = None # Extract the year from the second line

                    # Find a file with data
                    for subfile in tar.getnames()[1:]:
                        fp = tar.extractfile(subfile)

                        reader = csv.reader(fp)
                        header = reader.next()
                        try:
                            year = int(reader.next()[0])
                        except:
                            pass
                        fp.close()

                        if year is not None:
                            break

                    if year != 1981 and year != 1982:
                        print filename, "OKAY"
                        continue # Already done-- skip it!

                # Get the impact name
                name = filename[0:-7]

                # Given results from a generator
                def fix_year(generator):
                    first = generator.next()
                    yeardiff = 0
                    if first[0] == 1981 or first[0] == 1982:
                        yeardiff = 20

                    yield [first[0] - yeardiff] + first[2:]

                    for yearresult in generator:
                        yield [yearresult[0] - yeardiff] + yearresult[2:]

                print targetdir, name
                effect_bundle.make_tar_duplicate(name, os.path.join(targetdir, filename),
                                             effect_bundle.make_instabase(
                        effect_bundle.make(fix_year,
                                           effect_bundle.load_tar_make_generator(targetdir, name)), 2012),
                                                 targetdir=targetdir, collabel=header[1:])

    ### Result Generation Request Functions

    def make_byp_adapt(self):
        """Create constant p-value results with adaptation."""

        return self.make_byp(do_adapt=True)

    def make_byp_truehist(self):
        """Create constant p-value results for historical years."""

        return self.make_byp(ncdfset="truehist")

    def make_byp_noccscen(self):
        """Create constant p-value results for a no-climate-change scenario."""

        return self.make_byp(ncdfset="noccscen")

    def make_byp_modelhist(self):
        """Create constant p-value results for model results of historical years."""

        return self.make_byp(ncdfset="modelhist")

    def make_byp_mcpr(self):
        """Create Monte Carlo results for MCPR weather sets."""
        return self.make_byp(ncdfset="mcpr")

    def make_byp(self, do_adapt=False, ncdfset=None):
        """Create constant p-value results.

        Without any arguments, generates results for forecasted years
        across all models."""

        if do_adapt:
            # values in porder are the names of directories (processed in order)
            porder = ['pmed-adapt', 'pmed-adapt-xspace', 'pmed-adapt-xtime', 'pmed-adapt-xbaseline', 'pmed-adapt-xfuture']
            # What to do as an MC
            montevars = {'pmed-adapt': r'NOTHING HERE',
                         'pmed-adapt-xspace': r'_space_',
                         'pmed-adapt-xtime': r'_time_|_gamma$',
                         'pmed-adapt-xbaseline': '|'.join([ACRAController.adaptation[info]['baseline'] for info in ACRAController.adaptation]),
                         'pmed-adapt-xfuture': r'_future'}
            # values in pvals are the constant p-values to use for each directory
            pvals = {'pmed-adapt': .5, 'pmed-adapt-xspace': .5, 'pmed-adapt-xtime': .5, 'pmed-adapt-xbaseline': .5, 'pmed-adapt-xfuture': .5}
        else:
            # values in porder are the names of directories (processed in order)
            porder = ['pmed', 'plow', 'phigh']
            montevars = {'pmed': r'NOTHING HERE',
                         'plow': r'NOTHING HERE',
                         'phigh': r'NOTHING HERE'}
            # values in pvals are the constant p-values to use for each directory
            pvals = dict(pmed=.5, plow=.33333, phigh=.66667) # High = highly deletarious impact

        # Call make_results to generate the actual results
        # Extract the p-value from each dir when second argument is called
        self.make_results(porder, lambda pdir: { name: (pvals[pdir] if not re.match(montevars[pdir], name) else random.random()) for name in ACRAController.get_pval_names(do_adapt) }, do_adapt=do_adapt, ncdfset=ncdfset)

    def make_montecarlo_adapt(self):
        """Create Monte Carlo results with adaptation."""

        return self.make_montecarlo(do_adapt=True)

    def make_montecarlo_adapt_instant(self):
        """Create Monte Carlo results with adaptation with gamma = inf."""

        return self.make_montecarlo(do_adapt="instant")

    def make_montecarlo_adapt_instant_crime(self):
        """Create Monte Carlo results with adaptation with gamma = inf."""

        return self.make_montecarlo(do_adapt="instant-crime")

    def make_montecarlo_adapt_complete_crime(self):
        """Create Monte Carlo results with adaptation with all impacts to 0."""

        return self.make_montecarlo(do_adapt="complete")

    def make_montecarlo_adapt_complete_crime(self):
        """Create Monte Carlo results with adaptation with all impacts to 0."""

        return self.make_montecarlo(do_adapt="complete-crime")

    def make_montecarlo_truehist(self):
        """Create Monte Carlo results for historical data."""

        return self.make_montecarlo(ncdfset="truehist")

    def make_montecarlo_noccscen(self):
        """Create Monte Carlo results for a no-climate-change scenario."""

        return self.make_montecarlo(ncdfset="noccscen")

    def make_montecarlo_mcpr(self):
        """Create Monte Carlo results for MCPR weather sets."""
        return self.make_montecarlo(ncdfset="mcpr")

    def make_montecarlo(self, do_adapt=False, ncdfset=None):
        """Create Monte Carlo results.

        Without any arguments, generates results for forecasted years
        across all models."""

        if do_adapt:
            # Construct 25 batches of adapted results
            if do_adapt == "instant":
                batches = ["batch-adapt-instant-" + str(loop) for loop in range(25)]
            elif do_adapt == "complete":
                batches = ["batch-adapt-complete-" + str(loop) for loop in range(25)]
            elif do_adapt == "instant-crime":
                batches = ["batch-adapt-crime-instant-" + str(loop) for loop in range(25)]
            elif do_adapt == "complete-crime":
                batches = ["batch-adapt-crime-complete-" + str(loop) for loop in range(25)]
            else:
                batches = ["batch-adapt-" + str(loop) for loop in range(25)]
        elif ncdfset == "truehist":
            # Construct up to 2000 batches (each with 100 sets of generated years)
            batches = ["batch-" + str(loop) for loop in range(2000)]
        else:
            # Construct 25 batches of results
            batches = ["batch-" + str(loop) for loop in range(25)]

        print "Starting MC", ncdfset

        # Call make_results to generate the actual results
        # Construct full set of random p-values from each dir when second argument is called
        self.make_results(batches, lambda pdir: { name: random.random() for name in ACRAController.get_pval_names(do_adapt) }, do_adapt=do_adapt, ncdfset=ncdfset)

    ### Actual Result Generation

    def make_single(self, weatherdir, scenario=None, pval=None, do_adapt=False):
        # Collect the ACRA region definitions for regional aggregation
        regions = ACRAController.load_acra_regions()
        get_region = lambda fips: regions[fips] # passed to aggregate_tar

        variables = effect_bundle.get_arbitrary_variables(weatherdir)
        if 'tas' not in variables or 'pr' not in variables or 'tasmin' not in variables:
            print "Not all variables represented (tas, tasmin, tasmax, pr)."
            return

        if pval is None:
            pvals = { name: random.random() for name in ACRAController.get_pval_names(do_adapt) }
        else:
            pvals = { name: float(pval) for name in ACRAController.get_pval_names(do_adapt) }

        self.make_results_helper(variables, scenario, do_adapt, ".", pvals, get_region)

    def make_results(self, basedirs, make_pvals, do_adapt=False, only_one=False, ncdfset=None):
        """Generate impact bundles into a series of directories, as determined
        by the input weather datasets available.

        basedirs is a list of directories to generate
        <scenario>/<model>/<realization> subdirectories within.

        make_pvals generates a dictionary of p-values at which to
        evaluate the impact functions.

        if do_adapt is true, only a limited set of impacts with
        adaptation information will be calculated.  do_adapt can also
        be "instant", "complete", "instant-crime", or "complete-crime".

        if only_one is true, this function aborts after a single result set.

        ncdfset will be passed to find_ncdfs_allreal() to collect the
        input values.
        """

        # Collect the ACRA region definitions for regional aggregation
        regions = ACRAController.load_acra_regions()
        get_region = lambda fips: regions[fips] # passed to aggregate_tar

        for basedir in basedirs:
            print "Collecting results for", basedir

            # Iterate through all input sets
            for (variables, realization, scenario, model) in effect_bundle.find_ncdfs_allreal(ncdfset=ncdfset):
                print "Starting result set generation"
                # If ncdfset is a historical set, realization is list of years; scenario is 'noccscen' or 'truehist'

                # Ensure that this input set has the necessary variables
                if 'tas' not in variables or 'pr' not in variables or 'tasmin' not in variables:
                    effect_bundle.close_ncdf(variables)
                    continue

                # Get a dictionary of p-values
                pvals = make_pvals(basedir)

                # Define targetdir, the location for all outputs
                if model is not None:
                    targetdir = os.path.join(basedir, scenario, model, realization)
                else: # This is a historical run
                    targetdir = os.path.join(basedir, scenario)
                    # Save the years used in the realization
                    pvals['years'] = ','.join(map(str, realization))

                # Try to make this output directory
                try:
                    os.makedirs(targetdir) # if this directory already exists, fail!
                    # Write out all of the p-values to a file
                    results.make_pval_file(targetdir, pvals)
                except Exception, ex:
                    print ex
                    effect_bundle.close_ncdf(variables) # We can't generate these results
                    continue

                print targetdir

                self.make_results_helper(variables, scenario if model is not None else None, do_adapt, targetdir, pvals, get_region)

                if only_one:
                    return

    def make_results_helper(self, variables, scenario, do_adapt, targetdir, pvals, get_region):
        if do_adapt in ['instant-crime', 'complete-crime']:
            self.make_crime(variables, targetdir, pvals, get_region, do_adapt=do_adapt)
            return

        ## Generate the results of each kind

        # Agriculture:
        if not do_adapt:
            if scenario is not None:
                self.make_agriculture(variables, targetdir, pvals, ['rcp26', 'rcp45', 'rcp60', 'rcp85'].index(scenario) + 1, get_region)
            self.make_agriculture(variables, targetdir, pvals, 0, get_region)
        else:
            if scenario is not None:
                self.make_agriculture(variables, targetdir, pvals, ['rcp26', 'rcp45', 'rcp60', 'rcp85'].index(scenario) + 1, get_region, only_do='maize', do_adapt=do_adapt)
            self.make_agriculture(variables, targetdir, pvals, 0, get_region, only_do='maize', do_adapt=do_adapt)

        # Crime:
        self.make_crime(variables, targetdir, pvals, get_region, do_adapt=do_adapt)
        # Health/Mortality:
        self.make_health(variables['tas'], targetdir, pvals, get_region, do_adapt=do_adapt)

        if not do_adapt:
            self.make_health_age(variables['tas'], targetdir, pvals, get_region)

            # Labor Productivity:
            self.make_labor(variables['tasmax'], targetdir, pvals, get_region)

            # Residential Energy:
            self.make_energy(variables['tas'], targetdir, pvals, get_region)

        # Close any netCDFs opened for these calculations
        effect_bundle.close_ncdf(variables)

    ### Integrity Request Functions

    def check_only_integrity_montecarlo(self):
        """Check if Monte Carlo result sets are complete (but don't fix them
        if they aren't)."""

        return self.check_integrity_montecarlo(check_only=True)

    def check_only_integrity_byp(self):
        """Check if constant p-value result sets are complete (but don't fix
        them if they aren't).
        """

        return self.check_integrity_byp(check_only=True)

    def check_integrity_montecarlo(self, check_only=False):
        """Check if Monte Carlo result sets are complete."""

        # Collect the ACRA region definitions for regional aggregation
        regions = ACRAController.load_acra_regions()
        get_region = lambda fips: regions[fips] # passed to aggregate_tar

        # Iterate through all result set directories
        for batch, rcp, model, realization, pvals, targetdir in results.iterate_montecarlo('.', range(25)):
            # Check the integrity for this directory
            if check_only:
                self.check_only_integrity_single(targetdir, rcp, model, realization, pvals, get_region)
            else:
                self.check_integrity_single(targetdir, rcp, model, realization, pvals, get_region)

    def check_integrity_byp(self, check_only=False):
        """Check if constant p-value result sets are complete."""

        # Collect the ACRA region definitions for regional aggregation
        regions = ACRAController.load_acra_regions()
        get_region = lambda fips: regions[fips] # passed to aggregate_tar

        # Iterate through all result set directories
        for batch, rcp, model, realization, pvals, targetdir in results.iterate_byp('.'):
            # Check the integrity for this directory
            if check_only:
                self.check_only_integrity_single(targetdir, rcp, model, realization, pvals, get_region)
            else:
                self.check_integrity_single(targetdir, rcp, model, realization, pvals, get_region)

    def check_only_integrity_single(self, targetdir, rcp, model, realization, pvals, get_region):
        """Check the integrity of a single directory (but don't fix if not complete)."""

        # If this already has a check file, ignore it
        files = os.listdir(targetdir)
        if checked_file in files and checked_file_cge in files:
            return

        # Get the corresponding input variables and model
        (variables, scenario, fulmod) = effect_bundle.get_variables(realization, rcp, model)
        if 'tasmin' not in variables or 'pr' not in variables:
            # Flag this with a check-missvars file if it's missing variables
            print "Incomplete variable set."
            open(os.path.join(targetdir, 'check-missvars'), 'a').close()
            return

        # Construct a list of all the problems this result set has
        problems = []

        # Agriculture:
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('yields-maize')))
        problems.append(self.check_national_outofbounds(targetdir, 'yields-maize', 0, 20, 3))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('yields-wheat')))
        problems.append(self.check_national_outofbounds(targetdir, 'yields-wheat', 0, 20, 3))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('yields-grains')))
        problems.append(self.check_national_outofbounds(targetdir, 'yields-grains', 0, 20, 2))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('yields-cotton')))
        problems.append(self.check_national_outofbounds(targetdir, 'yields-cotton', 0, 20, 3))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('yields-oilcrop')))
        problems.append(self.check_national_outofbounds(targetdir, 'yields-oilcrop', 0, 20, 3))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('yields-total')))
        problems.append(self.check_national_outofbounds(targetdir, 'yields-total', 0, 20, 2))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('yields-maize-noco2')))
        problems.append(self.check_national_outofbounds(targetdir, 'yields-maize-noco2', 0, 20, 3))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('yields-wheat-noco2')))
        problems.append(self.check_national_outofbounds(targetdir, 'yields-wheat-noco2', 0, 20, 3))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('yields-grains-noco2')))
        problems.append(self.check_national_outofbounds(targetdir, 'yields-grains-noco2', 0, 20, 2))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('yields-cotton-noco2')))
        problems.append(self.check_national_outofbounds(targetdir, 'yields-cotton-noco2', 0, 20, 3))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('yields-oilcrop-noco2')))
        problems.append(self.check_national_outofbounds(targetdir, 'yields-oilcrop-noco2', 0, 20, 3))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('yields-total-noco2')))
        problems.append(self.check_national_outofbounds(targetdir, 'yields-total-noco2', 0, 20, 2))

        # Labor Productivity:
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('labor-high-productivity')))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('labor-low-productivity')))

        # Health:
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('health-mortality')))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('health-mortage-0-0')))
        problems.append(self.check_national_outofbounds(targetdir, 'health-mortage-0-0', -1, 1))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('health-mortage-1-44')))
        problems.append(self.check_national_outofbounds(targetdir, 'health-mortage-1-44', -.001, .001, checkall=True))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('health-mortage-45-64')))
        problems.append(self.check_national_outofbounds(targetdir, 'health-mortage-45-64', -1, 1))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('health-mortage-65-inf')))
        problems.append(self.check_national_outofbounds(targetdir, 'health-mortage-65-inf', -1, 1))
        problems.append(self.check_agriculture_redone(targetdir))

        # Check if we have any problems by this point
        problems = [p for p in problems if p is not None] # remove None's
        if len(problems) == 0:
            # No problems for the CGE-- flag it as good!
            open(os.path.join(targetdir, checked_file_cge), 'a').close()
            print targetdir + ": CGE GOOD"
        else:
            print targetdir + ':' + "\n".join(problems)

        # Crime:
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('crime-violent')))
        problems.append(self.check_directory_contains(targetdir, self.files_for_impact('crime-property')))

        problems = [p for p in problems if p is not None] # remove None's
        if len(problems) == 0:
            # No problems for anything-- flag it as good!
            open(os.path.join(targetdir, checked_file), 'a').close()
            print targetdir + ": GOOD"
        else:
            print targetdir + ':' + "\n".join(problems)

    def check_integrity_single(self, targetdir, rcp, model, realization, pvals, get_region):
        """Check the integrity of a single directory and fix if not complete."""

        print targetdir

        # If this already has a check file, or a checking file, ignore it
        files = os.listdir(targetdir)
        if checked_file in files or 'checking' in files:
            return

        # Ignore directories without a p-value info file
        if 'pvals.txt' not in files:
            return

        (variables, scenario, fulmod) = effect_bundle.get_variables(realization, rcp, model)
        if 'tasmin' not in variables or 'pr' not in variables:
            # Flag this with a check-missvars file if it's missing variables
            print "Incomplete variable set."
            open(os.path.join(targetdir, 'check-missvars'), 'a').close()
            return

        # Flag this as currently being checked
        open(os.path.join(targetdir, 'checking'), 'a').close()

        # Identify which CO2 trajectory this scenario follows
        co2col = ['rcp26', 'rcp45', 'rcp60', 'rcp85'].index(scenario) + 1

        files = os.listdir(targetdir)

        # Construct 'redo' functions to generate results that previously failed
        redo_agriculture = lambda only: lambda: self.make_agriculture(variables, targetdir, pvals, co2col, get_region, only_do=only)
        redo_agriculture_noco2 = lambda only: lambda: self.make_agriculture(variables, targetdir, pvals, 0, get_region, only_do=only)
        redo_crime = lambda: self.make_crime(variables, targetdir, pvals, get_region)
        redo_health = lambda: self.make_health(variables['tas'], targetdir, pvals, get_region)
        redo_health_age = lambda: self.make_health_age(variables['tas'], targetdir, pvals, get_region)
        redo_labor = lambda: self.make_labor(variables['tasmax'], targetdir, pvals, get_region)

        # Check each file, and redo it if it fails the check

        # Agriculture:
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('yields-maize')),
                            redo_agriculture('maize'))
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'yields-maize', 0, 20, 3),
                            redo_agriculture('maize'))
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('yields-wheat')),
                            redo_agriculture('wheat'))
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'yields-wheat', 0, 20, 3),
                            redo_agriculture('wheat'))
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('yields-grains')),
                            redo_agriculture('grains'))
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'yields-grains', 0, 20, 2),
                            redo_agriculture('grains'))
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('yields-cotton')),
                            redo_agriculture('cotton'))
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'yields-cotton', 0, 20, 3),
                            redo_agriculture('cotton'))
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('yields-oilcrop')),
                            redo_agriculture('oilcrop'))
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'yields-oilcrop', 0, 20, 3),
                            redo_agriculture('oilcrop'))
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('yields-total')),
                            redo_agriculture('total'))
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'yields-total', 0, 20, 2),
                            redo_agriculture('total'))
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('yields-maize-noco2')),
                            redo_agriculture_noco2('maize'))
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'yields-maize-noco2', 0, 20, 3),
                            redo_agriculture_noco2('maize'))
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('yields-wheat-noco2')),
                            redo_agriculture_noco2('wheat'))
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'yields-wheat-noco2', 0, 20, 3),
                            redo_agriculture_noco2('wheat'))
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('yields-grains-noco2')),
                            redo_agriculture_noco2('grains'))
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'yields-grains-noco2', 0, 20, 2),
                            redo_agriculture_noco2('grains'))
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('yields-cotton-noco2')),
                            redo_agriculture_noco2('cotton'))
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'yields-cotton-noco2', 0, 20, 3),
                            redo_agriculture_noco2('cotton'))
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('yields-oilcrop-noco2')),
                            redo_agriculture_noco2('oilcrop'))
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'yields-oilcrop-noco2', 0, 20, 3),
                            redo_agriculture_noco2('oilcrop'))
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('yields-total-noco2')),
                            redo_agriculture_noco2('total'))
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'yields-total-noco2', 0, 20, 2),
                            redo_agriculture_noco2('total'))

        # Labor:
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('labor-high-productivity')),
                            redo_labor)
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('labor-low-productivity')),
                            redo_labor)

        # Crime:
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('crime-violent')),
                            redo_crime)
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('crime-property')),
                            redo_crime)

        # Health:
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('health-mortality')),
                            redo_health)
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('health-mortage-0-0')),
                            redo_health_age)
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'health-mortage-0-0', -1, 1),
                            redo_health_age)
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('health-mortage-1-44')),
                            redo_health_age)
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'health-mortage-1-44', -.001, .001, checkall=True),
                            redo_health_age)
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('health-mortage-45-64')),
                            redo_health_age)
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'health-mortage-45-64', -1, 1),
                            redo_health_age)
        self.redo_as_needed(targetdir, lambda: self.check_directory_contains(targetdir, self.files_for_impact('health-mortage-65-inf')),
                            redo_health_age)
        self.redo_as_needed(targetdir, lambda: self.check_national_outofbounds(targetdir, 'health-mortage-65-inf', -1, 1),
                            redo_health_age)
        self.redo_as_needed(targetdir, lambda: self.check_agriculture_redone(targetdir), self.check_error)

        # Flag this as complete (and it will be now)
        open(os.path.join(targetdir, checked_file), 'a').close()
        # Remove the checking file
        os.remove(os.path.join(targetdir, 'checking'))

    def redo_as_needed(self, targetdir, func, redo):
        """Check if func() is satisfied.  If not, call redo() to regenerate
        the results, then check again."""

        # Check if this passes our test
        problem = func()
        if problem is None:
            # There is no problem!
            return

        print problem

        # Regenerate the result
        redo()

        # Check again if it passes the test
        problem = func()
        if problem is None:
            # We fixed the problem!
            return

        # The problem was not fixed by regenerating-- exit entirely
        print problem
        os.remove(os.path.join(targetdir, 'checking'))
        exit()

    ## Integrity tests

    def check_error(self):
        """Rather than simply returning a problem, actually throw an error."""

        raise ValueError('Check failed!')

    def check_directory_contains(self, targetdir, filesizedict):
        """Directory contents test: ensure that targetdir has each file in a
        dictionary, and that they are approximately the right sizes.

        filesizedict: {filename: approximate-size-in-bytes}"""

        # Check that all files are represented
        for filename in filesizedict:
            # Check that the file exists
            if not os.path.isfile(os.path.join(targetdir, filename)):
                return targetdir + ": " + filename + " does not exist"

            # Check that the size is between half and double what it should be
            size = os.path.getsize(os.path.join(targetdir, filename))
            if .5 * filesizedict[filename] > size or 2 * filesizedict[filename] < size:
                return targetdir + ": " + filename + " is unexpected size " + str(size)

        return None

    def files_for_impact(self, name):
        """Return a filesizedict (appropriate for passing to
        check_directory_contains()) for an impact by name."""

        # Agricultural files are have more columns, but fewer counties
        if name == 'yields-cotton' or name == 'yields-cotton-noco2':
            return {name + '.tar.gz': 9000000, name + '-state.tar.gz': 50000, name + '-region.tar.gz': 25000, name + '-national.tar.gz': 3200}
        elif name == 'yields-oilcrop' or name == 'yields-oicrop-noco2':
            return {name + '.tar.gz': 9000000, name + '-state.tar.gz': 90000, name + '-region.tar.gz': 25000, name + '-national.tar.gz': 3200}
        elif name == 'yields-maize' or name == 'yields-maize-noco2' or name == 'yields-wheat':
            return {name + '.tar.gz': 9000000, name + '-state.tar.gz': 100000, name + '-region.tar.gz': 33000, name + '-national.tar.gz': 3200}
        elif name == 'yields-wheat-noco2':
            return {name + '.tar.gz': 6000000, name + '-state.tar.gz': 100000, name + '-region.tar.gz': 35000, name + '-national.tar.gz': 3200}
        elif name == 'yields-grains' or name == 'yields-grains-noco2' or name == 'yields-total' or name == 'yields-total-noco2':
            return {name + '.tar.gz': 4500000, name + '-state.tar.gz': 85000, name + '-region.tar.gz': 25000, name + '-national.tar.gz': 2300}
        else:
            # All other impacts are of approximately uniform size
            return {name + '.tar.gz': 6000000, name + '-state.tar.gz': 100000, name + '-region.tar.gz': 28000, name + '-national.tar.gz': 2000}

    def check_national_outofbounds(self, targetdir, prefix, minval, maxval, column=2, checkall=False):
        """Check that all national results for an impact file are within bounds."""

        # Make sure the national file exists
        target = os.path.join(targetdir, prefix + "-national.tar.gz")
        if not os.path.exists(target):
            return "National results missing."

        # Find the national results in the tar ball
        with tarfile.open(target) as tar:
            fp = tar.extractfile(prefix + '-national/national.csv')

            # Read each line in the file
            reader = csv.reader(fp, delimiter=',')
            reader.next()
            for line in reader:
                # Check that there are enough columns
                if len(line) <= column:
                    return "Invalid number of columns"

                # Check that the value is within bounds
                value = float(line[column])
                if value < minval or value > maxval:
                    return prefix + ": National value out of bounds"

                # Stop after the first line, if checkall
                if not checkall:
                    break

    def check_agriculture_redone(self, targetdir):
        """Agriculture aggregation previously failed for some states (FIPS <
        10)-- this needed to be called before to fix it.
        """

        # Do we have a state file?
        files = os.listdir(targetdir)
        if 'yields-cotton-state.tar.gz' not in files:
            return "yields-cotton-state not found"

        # Check if alabama has cotton results (it should)
        with tarfile.open(os.path.join(targetdir, "yields-cotton-state.tar.gz")) as tar:
            try:
                fp = tar.extractfile('yields-cotton-state/01.csv')

                # Ensure that we have at least two lines
                ii = 0
                for line in fp:
                    if line.strip() == '':
                        return "Alabama file is empty"
                    ii += 1
                    if ii == 2:
                        return None

                return "Alabama file is empty"
            except:
                return "Alabama file does not exist"

    ### Agriculture Impact Generation

    def make_agriculture(self, ncdfs, targetdir=None, pvals=None, co2col=2, get_region=None, only_do=None, do_adapt=False):
        """Calculate percentage change of average yield.

        pvals: 15 elements: wheat-temp, maize-temp-east, maize-temp-west, maize-precip-east, maize-precip-west, cotton-temp, cotton-precip, soy-temp-east, soy-temp-west, soy-precip-east, soy-precip-west, wheat-co2, maize-co2, cotton-co2, soy-co2

        if only_do is None, generate all crops (otherwise, just one)
        """

        # Construct a scaling function, based on the specified co2col:
        # 0 = no CO2, 1 = rcp 2.6, 2 = rcp 4.5, 3 = rcp 6.0, 4 = rcp 8.5
        co2scale = ACRAController.make_co2scale(co2col)
        if co2col == 0:
            suffix = '-noco2'
        else:
            suffix = ''

        # Construct county-specific results

        if only_do is None or only_do == 'maize':
            print "maize"
            # Add a 2012 baseline year to make_maize_generator
            # Output file columns: relative (to 2012), output (after AR()), production (from function)
            # tas needed for adaptation
            if do_adapt == 'instant' or do_adapt == 'complete':
                print "Skip agriculture for instant and complete adaptation."
                return

            effect_bundle.make_tar_ncdf('yields-maize' + suffix, ncdfs, ['tas', 'tasmin', 'tasmax', 'pr'],
                                        effect_bundle.make_instabase(ACRAController.make_maize_generator(pvals, co2scale, do_adapt=do_adapt), 2012), targetdir, collabel=['relative', 'output', 'production'])
            if do_adapt == True:
                effect_bundle.make_tar_ncdf('yields-maize-gddkdd' + suffix, ncdfs, ['tas', 'tasmin', 'tasmax', 'pr'],
                                            effect_bundle.make_instabase(ACRAController.make_maize_generator(pvals, co2scale, do_adapt='gddkdd'), 2012), targetdir, collabel=['relative', 'output', 'production'])

        if only_do is None or only_do == 'wheat':
            print "wheat"
            # Add a 2012 baseline year to make_wheat_generator
            # Output file columns: relative (to 2012), output (after AR()), production (from function)
            effect_bundle.make_tar_ncdf('yields-wheat' + suffix, ncdfs, ['tas'],
                                        effect_bundle.make_instabase(ACRAController.make_wheat_generator(pvals, co2scale), 2012), targetdir, collabel=['relative', 'output', 'production'])

        if only_do is None or only_do == 'grains':
            print "grains"
            # Load the pre-existing impacts for wheat and maize
            # Combine according to calorie counts
            # Add a 2012 baseline year to make_wheat_generator
            effect_bundle.make_tar_ncdf('yields-grains' + suffix, ncdfs, ['tas'],
                                        effect_bundle.make_instabase(
                                            agriculture.make_generator_combo_crops([
                                                effect_bundle.load_tar_make_generator(targetdir, 'yields-wheat' + suffix, column=2),
                                                effect_bundle.load_tar_make_generator(targetdir, 'yields-maize' + suffix, column=2)], ['wheat-planted', 'maize-planted'], [1615., 1690.]), 2012), targetdir, collabel=['relative', 'output', 'production'])

        if only_do is None or only_do == 'cotton':
            print "cotton"
            # Add a 2012 baseline year to make_cotton_generator
            # Output file columns: relative (to 2012), output (after AR()), production (from function)
            effect_bundle.make_tar_ncdf('yields-cotton' + suffix, ncdfs, ['tasmin', 'tasmax', 'pr'],
                                        effect_bundle.make_instabase(ACRAController.make_cotton_generator(pvals, co2scale), 2012), targetdir, collabel=['relative', 'output', 'production'])

        if only_do is None or only_do == 'oilcrop':
            print "oilcrop"
            # Add a 2012 baseline year to make_oilcrop_generator
            # Output file columns: relative (to 2012), output (after AR()), production (from function)
            effect_bundle.make_tar_ncdf('yields-oilcrop' + suffix, ncdfs, ['tasmin', 'tasmax', 'pr'],
                                        effect_bundle.make_instabase(ACRAController.make_oilcrop_generator(pvals, co2scale), 2012), targetdir, collabel=['relative', 'output', 'production'])

        if only_do is None or only_do == 'total':
            print "total"
            # Load the pre-existing impacts for wheat, maize, cotton, and oilcrop
            # Combine according to total production
            # Add a 2012 baseline year to make_wheat_generator
            effect_bundle.make_tar_ncdf('yields-total' + suffix, ncdfs, ['tas'],
                                        effect_bundle.make_instabase(agriculture.make_generator_combo_crops([
                                            effect_bundle.load_tar_make_generator(targetdir, 'yields-wheat' + suffix, column=2),
                                            effect_bundle.load_tar_make_generator(targetdir, 'yields-maize' + suffix, column=2),
                                            effect_bundle.load_tar_make_generator(targetdir, 'yields-cotton' + suffix, column=2),
                                            effect_bundle.load_tar_make_generator(targetdir, 'yields-oilcrop' + suffix, column=2)], ['wheat-planted', 'maize-planted', 'cotton-planted', 'soy-planted'], [1, 1, 1, 1]), 2012), targetdir, collabel=['relative', 'output', 'production'])

        # Don't aggregate if results were being passed to a function
        if hasattr(targetdir, '__call__'):
            return

        # Aggregate results to state, regional, and national levels
        # Maize:
        if only_do is None or only_do == 'maize':
            agriculture.aggregate_tar_with_scale_file('yields-maize' + suffix, ['maize-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=get_region)
            agriculture.aggregate_tar_with_scale_file('yields-maize' + suffix, ['maize-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=True)
            agriculture.aggregate_tar_with_scale_file('yields-maize' + suffix, ['maize-planted'], [1], targetdir, collabel=['relative', 'output', 'production'])
            if do_adapt:
                agriculture.aggregate_tar_with_scale_file('yields-maize-gddkdd' + suffix, ['maize-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=get_region)
                agriculture.aggregate_tar_with_scale_file('yields-maize-gddkdd' + suffix, ['maize-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=True)
                agriculture.aggregate_tar_with_scale_file('yields-maize-gddkdd' + suffix, ['maize-planted'], [1], targetdir, collabel=['relative', 'output', 'production'])

        # Wheat:
        if only_do is None or only_do == 'wheat':
            agriculture.aggregate_tar_with_scale_file('yields-wheat' + suffix, ['wheat-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=get_region)
            agriculture.aggregate_tar_with_scale_file('yields-wheat' + suffix, ['wheat-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=True)
            agriculture.aggregate_tar_with_scale_file('yields-wheat' + suffix, ['wheat-planted'], [1], targetdir, collabel=['relative', 'output', 'production'])

        # Grains (maize + wheat)
        if only_do is None or only_do == 'grains':
            agriculture.aggregate_tar_with_scale_file('yields-grains' + suffix, ['maize-planted','wheat-planted'], [1690.,1615.], targetdir, collabel=['relative', 'output', 'production'], get_region=get_region)
            agriculture.aggregate_tar_with_scale_file('yields-grains' + suffix, ['maize-planted','wheat-planted'], [1690.,1615.], targetdir, collabel=['relative', 'output', 'production'], get_region=True)
            agriculture.aggregate_tar_with_scale_file('yields-grains' + suffix, ['maize-planted','wheat-planted'], [1690.,1615.], targetdir, collabel=['relative', 'output', 'production'])

        # Cotton:
        if only_do is None or only_do == 'cotton':
            agriculture.aggregate_tar_with_scale_file('yields-cotton' + suffix, ['cotton-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=get_region)
            agriculture.aggregate_tar_with_scale_file('yields-cotton' + suffix, ['cotton-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=True)
            agriculture.aggregate_tar_with_scale_file('yields-cotton' + suffix, ['cotton-planted'], [1], targetdir, collabel=['relative', 'output', 'production'])

        # Oilcrop:
        if only_do is None or only_do == 'oilcrop':
            agriculture.aggregate_tar_with_scale_file('yields-oilcrop' + suffix, ['soy-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=get_region)
            agriculture.aggregate_tar_with_scale_file('yields-oilcrop' + suffix, ['soy-planted'], [1], targetdir, collabel=['relative', 'output', 'production'], get_region=True)
            agriculture.aggregate_tar_with_scale_file('yields-oilcrop' + suffix, ['soy-planted'], [1], targetdir, collabel=['relative', 'output', 'production'])

        # Total (maize + wheat + cotton + oilcrop)
        if only_do is None or only_do == 'total':
            agriculture.aggregate_tar_with_scale_file('yields-total' + suffix, ['wheat-planted', 'maize-planted', 'cotton-planted', 'soy-planted'], [1, 1, 1, 1], targetdir, collabel=['relative', 'output', 'production'], get_region=get_region)
            agriculture.aggregate_tar_with_scale_file('yields-total' + suffix, ['wheat-planted', 'maize-planted', 'cotton-planted', 'soy-planted'], [1, 1, 1, 1], targetdir, collabel=['relative', 'output', 'production'], get_region=True)
            agriculture.aggregate_tar_with_scale_file('yields-total' + suffix, ['wheat-planted', 'maize-planted', 'cotton-planted', 'soy-planted'], [1, 1, 1, 1], targetdir, collabel=['relative', 'output', 'production'])

    @staticmethod
    def make_co2scale(co2col=2):
        """Construct a function that scales a yield result by a CO2 result,
        given then level of CO2 in a given year."""

        # If co2col = 0, don't use co2scaling
        if co2col == 0:
            return lambda x, y, year: x # Apply no fertilization

        # Extract all CO2 levels by year
        co2conc = {}
        with open(aggregator.__path__[0] + "/lib/acra/climate/co2conc.csv") as fp:
            reader = csv.reader(fp, delimiter=',')
            reader.next()
            for row in reader:
                # Format of co2conc.csv is YEAR,RCP26,RCP45,RCP60,RCP85
                co2conc[int(row[0])] = float(row[co2col])

        co2base = co2conc[2012] # always compare to a base CO2 level from 2012

        # For every additional 100 PPM, add y as a percent increase
        co2scale = lambda x, y, year: x * (1 + ((co2conc[year] - co2base) / 100) * y / 100.0)

        return co2scale

    @staticmethod
    def make_maize_generator(pvals, co2scale, do_adapt=False):
        # do_adapt can be False, True, 'gddkdd', 'instant', or 'complete'
        # Get the impact curve as either a Model or a model ID
        if do_adapt and do_adapt != 'gddkdd': # default is complete
            model_east_tas = ACRAController.make_adapting_curve('maize_east', pvals, do_adapt)
        elif do_adapt == 'gddkdd':
            model_east_tas = ACRAController.make_adapting_curve('maize_east_gddkdd', pvals, True)
        else:
            model_east_tas = ACRAController.models['maize_east_tas_model']

        # Call make_daily_degreedaybinslog_conditional to calculate effect,
        #   conditional on being East and not Florida
        # Need to scale all inputs by .01 because functions use hundreds of degree days
        # Scale by CO2 as needed
        # Make a running average of the results, based on Sol's AR() calculation
        return effect_bundle.make_runaverage(agriculture.make_modelscale_byyear(
            agriculture.make_daily_degreedaybinslog_conditional('maize', [model_east_tas, ACRAController.models['maize_west_tas_model']], [ACRAController.models['maize_east_pr_model'], ACRAController.models['maize_west_pr_model']], lambda fips, lat, lon: 0 if lon > -100 and fips[0:2] != '12' else 1, .01, map(lambda p: 1 - p, [pvals['maize_east_tas_model'], pvals['maize_west_tas_model'], pvals['maize_east_pr_model'], pvals['maize_west_pr_model']])),
            ACRAController.models['maize_co2_model'], 1 - pvals['maize_co2_model'], co2scale), [1, 1, 1], [0.21, 0.28, 0.51], unshift=True)

    @staticmethod
    def make_wheat_generator(pvals, co2scale):
        # Call make_generator_single_crop to calculate effect
        # Scale by CO2 as needed
        # Make a running average of the results, based on Sol's AR() calculation
        return effect_bundle.make_runaverage(agriculture.make_modelscale_byyear(
            agriculture.make_generator_single_crop('wheat', ACRAController.models['wheat_tas_model'], 1 - pvals['wheat_tas_model']),
                ACRAController.models['wheat_co2_model'], 1 - pvals['wheat_co2_model'], co2scale), [1, 1, 1], [0.21, 0.28, 0.51], unshift=True)

    @staticmethod
    def make_cotton_generator(pvals, co2scale):
        # Call make_daily_degreedaybinslog to calculate effect,
        # Need to scale all inputs by .01 because functions use hundreds of degree days
        # Scale by CO2 as needed
        # Make a running average of the results, based on Sol's AR() calculation
        return effect_bundle.make_runaverage(agriculture.make_modelscale_byyear(
            agriculture.make_daily_degreedaybinslog('cotton', ACRAController.models['cotton_tas_model'], ACRAController.models['cotton_pr_model'], .01, map(lambda p: 1 - p, [pvals['cotton_tas_model'], pvals['cotton_pr_model']])),
            remote.view_model('url', ACRAController.models['cotton_co2_url']),
            1 - pvals['cotton_co2_url'], co2scale), [1, 1, 1], [0.21, 0.28, 0.51], unshift=True)

    @staticmethod
    def make_oilcrop_generator(pvals, co2scale):
        # Call make_daily_degreedaybinslog_conditional to calculate effect,
        #   conditional on being East and not Florida
        # Need to scale all inputs by .01 because functions use hundreds of degree days
        # Scale by CO2 as needed
        # Make a running average of the results, based on Sol's AR() calculation
        return effect_bundle.make_runaverage(agriculture.make_modelscale_byyear(
            agriculture.make_daily_degreedaybinslog_conditional('soy', [ACRAController.models['soy_east_tas_model'], ACRAController.models['soy_west_tas_model']], [ACRAController.models['soy_east_pr_model'], ACRAController.models['soy_west_pr_model']], lambda fips, lat, lon: 0 if lon > -100 and fips[0:2] != '12' else 1, .01, map(lambda p: 1 - p, [pvals['soy_east_tas_model'], pvals['soy_west_tas_model'], pvals['soy_east_pr_model'], pvals['soy_west_pr_model']])),
            ACRAController.models['soy_co2_model'], 1 - pvals['soy_co2_model'], co2scale), [1, 1, 1, 1], [0.1, 0.19, 0.52, 0.2], unshift=True)

    ### Crime Impact Generation

    def make_crime(self, ncdf=None, targetdir=None, pvals=None, get_region=None, do_adapt=False):
        """Generate change in crime rate as percent of the average.

        ncdf can be a single 'tas' filename
        """

        # For debugging purpose, if no ncdf passed in, use default tas file
        if ncdf is None:
            ncdf = effect_bundle.default_weather_ncdf

        print "crime-violent"
        # Add a 2012 baseline year to make_violent_crime_generator
        effect_bundle.make_tar_ncdf('crime-violent', ncdf, ['tasmax', 'pr'],
                                    effect_bundle.make_instabase(self.make_violent_crime_generator(pvals, do_adapt=do_adapt), 2012), targetdir, collabel=['relative', 'impact'])
        if not do_adapt:
            effect_bundle.make_tar_ncdf('crime-violent-adaptable', ncdf, ['tasmax', 'pr'],
                                        effect_bundle.make_instabase(self.make_violent_crime_generator(pvals, do_adapt='compare'), 2012), targetdir, collabel=['relative', 'impact'])

        # Aggregate results to state, regional, and national levels
        if not hasattr(targetdir, '__call__'):
            ACRAController.crime_aggregate_tar('crime-violent', targetdir, collabel=['relative', 'impact'], get_region=get_region)
            ACRAController.crime_aggregate_tar('crime-violent', targetdir, collabel=['relative', 'impact'], get_region=True)
            ACRAController.crime_aggregate_tar('crime-violent', targetdir, collabel=['relative', 'impact'])
            if not do_adapt:
                ACRAController.crime_aggregate_tar('crime-violent-adaptable', targetdir, collabel=['relative', 'impact'], get_region=get_region)
                ACRAController.crime_aggregate_tar('crime-violent-adaptable', targetdir, collabel=['relative', 'impact'], get_region=True)
                ACRAController.crime_aggregate_tar('crime-violent-adaptable', targetdir, collabel=['relative', 'impact'])

        print "crime-property"
        # Add a 2012 baseline year to make_property_crime_generator
        effect_bundle.make_tar_ncdf('crime-property', ncdf, ['tasmax', 'pr'],
                                    effect_bundle.make_instabase(self.make_property_crime_generator(pvals, do_adapt=do_adapt), 2012), targetdir, collabel=['relative', 'impact'])
        if not do_adapt:
            effect_bundle.make_tar_ncdf('crime-property-adaptable', ncdf, ['tasmax', 'pr'],
                                        effect_bundle.make_instabase(self.make_property_crime_generator(pvals, do_adapt='compare'), 2012), targetdir, collabel=['relative', 'impact'])

        # Aggregate results to state, regional, and national levels
        if not hasattr(targetdir, '__call__'):
            ACRAController.crime_aggregate_tar('crime-property', targetdir, collabel=['relative', 'impact'], get_region=get_region)
            ACRAController.crime_aggregate_tar('crime-property', targetdir, collabel=['relative', 'impact'], get_region=True)
            ACRAController.crime_aggregate_tar('crime-property', targetdir, collabel=['relative', 'impact'])
            if not do_adapt:
                ACRAController.crime_aggregate_tar('crime-property-adaptable', targetdir, collabel=['relative', 'impact'], get_region=get_region)
                ACRAController.crime_aggregate_tar('crime-property-adaptable', targetdir, collabel=['relative', 'impact'], get_region=True)
                ACRAController.crime_aggregate_tar('crime-property-adaptable', targetdir, collabel=['relative', 'impact'])

    def make_violent_crime_generator(self, pvals, do_adapt=False):
        """Generate violent crime impacts."""

        if do_adapt in [True, 'instant', 'complete', 'instant-crime', 'complete-crime']:
            # Construct a general adapting curve
            model_tasmax = ACRAController.make_adapting_curve('crime_violent', pvals, do_adapt)
        elif do_adapt == 'compare':
            # Not an adaptation run, but a comparison to one
            model_tasmax = remote.view_model('url', ACRAController.models['crime_violent_adaptable_tasmax_url'])
        elif do_adapt:
            # Construct a specific adapting curve (name specified as a key in adaptation dictionary)
            model_tasmax = remote.view_model('url', ACRAController.adaptation['crime_violent']['space'][do_adapt])
        else:
            # Collect the temperature model from this URL
            model_tasmax = remote.view_model('url', ACRAController.models['crime_violent_tasmax_url'])

        # Collect the precipitation model from this URL
        model_pr = remote.view_model('url', ACRAController.models['crime_violent_pr_url'])

        # The result is the product of two percent increases
        return effect_bundle.make_product(['tasmax', 'pr'], [
            daily.make_daily_bymonthdaybins(model_tasmax, lambda x: 1 + x / 100.0, pvals['crime_violent_tasmax_url']),
            daily.make_daily_bymonthdaybins(model_pr, lambda x: 1 + x / 100.0, pvals['crime_violent_pr_url'], lambda x: x * (x > 0))]) # Ignore negative precipitation

    def make_property_crime_generator(self, pvals, do_adapt=False):
        if do_adapt in [True, 'instant', 'complete', 'instant-crime', 'complete-crime']:
            # Construct a general adapting curve
            model_tasmax = ACRAController.make_adapting_curve('crime_property', pvals, do_adapt)
        elif do_adapt == 'compare':
            # Not an adaptation run, but a comparison to one
            model_tasmax = remote.view_model('url', ACRAController.models['crime_property_adaptable_tasmax_url'])
        elif do_adapt:
            # Construct a specific adapting curve (name specified as a key in adaptation dictionary)
            model_tasmax = remote.view_model('url', ACRAController.adaptation['crime_property']['space'][do_adapt])
        else:
            # Collect the temperature model from this URL
            model_tasmax = remote.view_model('url', ACRAController.models['crime_property_tasmax_url'])

        # Collect the temperature model from this URL
        model_pr = remote.view_model('url', ACRAController.models['crime_property_pr_url'])

        # The result is the product of two percent increases
        return effect_bundle.make_product(['tasmax', 'pr'], [
            daily.make_daily_bymonthdaybins(model_tasmax, lambda x: 1 + x / 100.0, pvals['crime_property_tasmax_url']),
            daily.make_daily_bymonthdaybins(model_pr, lambda x: 1 + x / 100.0, pvals['crime_property_pr_url'], lambda x: x * (x > 0))]) # Ignore negative precipitation

    def make_energy(self, ncdf=None, targetdir=None, pvals=None, get_region=None):
        """Calculate percent change in household energy consumption.

        ncdf can be a single 'tas' filename
        """

        # For debugging purpose, if no ncdf passed in, use default tas file
        if ncdf is None:
            ncdf = effect_bundle.default_weather_ncdf

        # Add a 2012 baseline year to make_daily_bymonthdaybins result (exponentiated)
        effect_bundle.make_tar_ncdf('energy-residential', ncdf, 'tas',
                                    effect_bundle.make_instabase(daily.make_daily_bymonthdaybins(ACRAController.models['energy_tas_model'], lambda x: math.exp(x), pvals['energy_tas_model']), 2012), targetdir)

        # Aggregate results to state, regional, and national levels
        if not hasattr(targetdir, '__call__'):
            ACRAController.population_aggregate_tar('energy-residential', targetdir, get_region=get_region)
            ACRAController.population_aggregate_tar('energy-residential', targetdir, get_region=True)
            ACRAController.population_aggregate_tar('energy-residential', targetdir)

    def make_health(self, ncdf=None, targetdir=None, pvals=None, get_region=None, do_adapt=False):
        """Calculate absolute change in annual mortality rate.

        ncdf can be a single 'tas' filename
        """

        # For debugging purpose, if no ncdf passed in, use default tas file
        if ncdf is None:
            ncdf = effect_bundle.default_weather_ncdf

        # Impact gives change in log(mortality) =(approx)= change in mortality
        # Then scale by current mortality to get change in deaths per person
        effect_bundle.make_tar_ncdf('health-mortality', ncdf, 'tas',
                                    effect_bundle.make_instabase(
                                        effect_bundle.make_scale(self.make_health_mortality_generator(pvals, do_adapt=do_adapt), mortality.load_mortality_rates()), 2012, lambda x, y: x - y), targetdir, collabel=["addlrate", 'output'])

        # Aggregate results to state, regional, and national levels
        if not hasattr(targetdir, '__call__'):
            ACRAController.population_aggregate_tar('health-mortality', targetdir, collabel=["addlrate", 'output'], get_region=get_region)
            ACRAController.population_aggregate_tar('health-mortality', targetdir, collabel=["addlrate", 'output'], get_region=True)
            ACRAController.population_aggregate_tar('health-mortality', targetdir, collabel=["addlrate", 'output'])

    def make_health_mortality_generator(self, pvals, do_adapt=False):
        # Get the impact curve as a Model
        if do_adapt:
            model_tas = ACRAController.make_adapting_curve('mortality', pvals, do_adapt)
        else:
            model_tas = remote.view_model('url', ACRAController.models['mortality_tas_url'])

        # Calculate result by computing the number of days within temperature bins
        return daily.make_daily_yearlydaybins(model_tas, pval=pvals['mortality_tas_url'])

    def make_health_age(self, ncdf=None, targetdir=None, pvals=None, get_region=None):
        """Calculate absolute change in annual mortality rate for each of four
        age groups.

        ncdf can be a single 'tas' filename
        """

        # For debugging purpose, if no ncdf passed in, use default tas file
        if ncdf is None:
            ncdf = effect_bundle.default_weather_ncdf

        # Collect the relevant information
        bounds = ["0-0", "1-44", "45-64", "65-inf"] # bounds used as a file suffix and to pass to load_mortality_age_rates
        models = [ACRAController.models['mortality_0_0_tas_model'], ACRAController.models['mortality_1_44_tas_model'], ACRAController.models['mortality_45_64_tas_model'], ACRAController.models['mortality_65_inf_tas_model']] # all model IDs
        modelpvals = [pvals['mortality_0_0_tas_model'], pvals['mortality_1_44_tas_model'], pvals['mortality_45_64_tas_model'], pvals['mortality_65_inf_tas_model']] # p-value for each model

        # Loop through age groups
        for ii in range(4):
            # Calculate the effect with make_daily_yearlydaybins
            # Scale by age-specific mortality rates to get deaths-per-person
            # Report results as changes from the 2012 mortality rate
            # Output file columns: addlrate (additional deaths-per-person), output (impact result)
            effect_bundle.make_tar_ncdf('health-mortage-' + bounds[ii], ncdf, 'tas',
                                        effect_bundle.make_instabase(
                    effect_bundle.make_scale(daily.make_daily_yearlydaybins(models[ii], pval=modelpvals[ii]), mortality.load_mortality_age_rates(bounds[ii])), 2012, lambda x, y: x - y), targetdir, collabel=["addlrate", 'output'])

            # Aggregate results to state, regional, and national levels
            if not hasattr(targetdir, '__call__'):
                ACRAController.population_aggregate_tar('health-mortage-' + bounds[ii], targetdir, collabel=["addlrate", 'output'], get_region=get_region)
                ACRAController.population_aggregate_tar('health-mortage-' + bounds[ii], targetdir, collabel=["addlrate", 'output'], get_region=True)
                ACRAController.population_aggregate_tar('health-mortage-' + bounds[ii], targetdir, collabel=["addlrate", 'output'])

    def make_labor(self, ncdf=None, targetdir=None, pvals=None, get_region=None):
        """Calculate relative productivity for labor.

        ncdf can be a single 'tasmax' filename
        """

        # For debugging purpose, if no ncdf passed in, use default tas file
        if ncdf is None:
            ncdf = effect_bundle.default_weather_ncdf # Note: we actually want a tasmax file

        # Add a 2012 baseline year to make_labor_high_generator result
        effect_bundle.make_tar_ncdf('labor-high-productivity', ncdf, 'tasmax',
                                    effect_bundle.make_instabase(self.make_labor_high_generator(pvals),
                                                                 2012), targetdir, collabel=['fraction', 'output'])

        # Aggregate results to state, regional, and national levels
        if not hasattr(targetdir, '__call__'):
            ACRAController.labor_aggregate_tar('labor-high-productivity', targetdir, True, collabel=['fraction', 'output'], get_region=get_region)
            ACRAController.labor_aggregate_tar('labor-high-productivity', targetdir, True, collabel=['fraction', 'output'], get_region=True)
            ACRAController.labor_aggregate_tar('labor-high-productivity', targetdir, True, collabel=['fraction', 'output'])

        # Add a 2012 baseline year to make_labor_low_generator result
        effect_bundle.make_tar_ncdf('labor-low-productivity', ncdf, 'tasmax',
                                    effect_bundle.make_instabase(self.make_labor_low_generator(pvals),
                                                                 2012), targetdir, collabel=['fraction', 'output'])

        # Aggregate results to state, regional, and national levels
        if not hasattr(targetdir, '__call__'):
            ACRAController.labor_aggregate_tar('labor-low-productivity', targetdir, False, collabel=['fraction', 'output'], get_region=get_region)
            ACRAController.labor_aggregate_tar('labor-low-productivity', targetdir, False, collabel=['fraction', 'output'], get_region=True)
            ACRAController.labor_aggregate_tar('labor-low-productivity', targetdir, False, collabel=['fraction', 'output'])

    def make_labor_high_generator(self, pvals):
        """Calculate the effect on high-risk labor."""

        # 7.67 hr/day for high-risk
        work_per_month = 7.67*365/12

        # Collect the tasmax values within an average month
        # Convert from minutes lots to relative hours
        return daily.make_daily_bymonthdaybins(ACRAController.models['labor_high_tasmax_model'], lambda x: (work_per_month + (x/60)) / work_per_month, 1 - pvals['labor_high_tasmax_model'])

    def make_labor_low_generator(self, pvals):
        """Calculate the effect on low-risk labor."""

        # 6.92 hr/day for low-risk
        work_per_month = 6.92*365/12

        # Collect the tasmax values within an average month
        # Convert from minutes lots to relative hours
        return daily.make_daily_bymonthdaybins(ACRAController.models['labor_low_tasmax_model'], lambda x: (work_per_month + (x/60)) / work_per_month, 1 - pvals['labor_low_tasmax_model'])

    def make_labor_total(self, targetdir):
        """Generate a impact bundle for all labor (low and high risk)."""

        # Set aside global scale dictionaries to be filled in by callbacks
        global scales_low, scales_high
        scales_low = None # total jobs in low-risk
        scales_high = None # total jobs in high-risk

        # Fill in the scales_low dictionary
        def callback_low(s):
            global scales_low
            scales_low = s

        # Fill in the scales_high dictionary
        def callback_high(s):
            global scales_high
            scales_high = s

        # Get the aggregate weights (total jobs)
        ACRAController.labor_aggregate_tar(None, targetdir, False, callback=callback_low)
        ACRAController.labor_aggregate_tar(None, targetdir, True, callback=callback_high)

        # Load low- and high-risk productivity effects
        # Construct a weighted average of these effects (based on # jobs)
        # Report results relative to 2012
        effect_bundle.make_tar_dummy('labor-total-productivity', aggregator.__path__[0] + "/lib/acra",
                                    effect_bundle.make_instabase(effect_bundle.make_weighted_average([
                        effect_bundle.load_tar_make_generator(targetdir, 'labor-low-productivity'),
                        effect_bundle.load_tar_make_generator(targetdir, 'labor-high-productivity')], [
                                                scales_low, scales_high]), 2012), targetdir)

        # Collect the ACRA region definitions for regional aggregation
        regions = ACRAController.load_acra_regions()
        get_region = lambda fips: regions[fips] # passed to aggregate_tar

        # Aggregate results to state, regional, and national levels
        ACRAController.labor_total_aggregate_tar('labor-total-productivity', targetdir, collabel=['fraction', 'output'], get_region=get_region)
        ACRAController.labor_total_aggregate_tar('labor-total-productivity', targetdir, collabel=['fraction', 'output'], get_region=True)
        ACRAController.labor_total_aggregate_tar('labor-total-productivity', targetdir, collabel=['fraction', 'output'])

    ### Aggregation Functions

    @staticmethod
    def crime_aggregate_tar(name, targetdir, collabel="fraction", get_region=None):
        if name == 'crime-violent':
            crime_type = 0
        else:
            crime_type = 1

        scales = crime.load_crime_rates(crime_type, census)

        effect_bundle.aggregate_tar(name, scales, targetdir, collabel=collabel, get_region=get_region, report_all=True)

    @staticmethod
    def labor_aggregate_tar(name, targetdir, high_risk, collabel="fraction", get_region=None, callback=None):
        """Calls either callback or aggregate_tar with a scaling dictionary of
        the number of crimes by county.

        Constructs {fips => scale}
        """

        # Open up the labor jobs data
        with open(aggregator.__path__[0] + "/lib/acra/labor/lab_cty_00_05_sum.csv") as countyfp:
            reader = csv.reader(countyfp, delimiter=',')
            reader.next() # skip header

            # Fill in scale dictionary with all counties
            scales = {}
            for row in reader:
                fips = row[0]
                if high_risk:
                    # high risk is agriculture, forestry, fishing, and hunting; mining; construction; manufacturing; and transportation and utilities industries
                    total = sum(map(float, [row[1], row[2], row[3], row[4], row[5], row[8]]))
                else:
                    # low risk is the remaining
                    total = sum(map(float, [row[6], row[7]] + row[9:]))

                scales[fips] = total

        # Call with this scale dictionary
        if callback is None:
            effect_bundle.aggregate_tar(name, scales, targetdir, collabel=collabel, get_region=get_region, report_all=True)
        else:
            callback(scales)

    @staticmethod
    def labor_total_aggregate_tar(name, targetdir, collabel="fraction", get_region=None, callback=None):
        """Generate scaling dictionary ({fips => scale}) for both low and high
        risk, and then either call callback or aggregate_tar with it."""

        # Set aside global scale dictionaries to be filled in by callbacks
        global scales_low, scales_high
        scales_low = None # total jobs in low-risk
        scales_high = None # total jobs in high-risk

        # Fill in the scales_low dictionary
        def callback_low(s):
            global scales_low
            scales_low = s

        # Fill in the scales_high dictionary
        def callback_high(s):
            global scales_high
            scales_high = s

        # Get the aggregate weights (total jobs)
        ACRAController.labor_aggregate_tar(None, targetdir, False, callback=callback_low)
        ACRAController.labor_aggregate_tar(None, targetdir, True, callback=callback_high)

        # Merge these to get the total jobs per county
        scales_total = {}
        for fips in scales_low:
            scales_total[fips] = scales_low[fips]
        for fips in scales_high:
            if fips in scales_total:
                scales_total[fips] += scales_high[fips]
            else:
                scales_total[fips] = scales_high(fips)

        # Call with this scale dictionary
        if callback is None:
            effect_bundle.aggregate_tar(name, scales_total, targetdir, collabel=collabel, get_region=get_region, report_all=True)
        else:
            callback(scales_total)

    @staticmethod
    def population_aggregate_tar(name, targetdir, collabel="fraction", get_region=None, callback=None):
        """Constructs {fips => scale}"""

        scales = census.get_populations_2010()

        if callback is None:
            effect_bundle.aggregate_tar(name, scales, targetdir, collabel=collabel, get_region=get_region, report_all=True)
        else:
            callback(scales)

    ### Adaptation Calculations

    @staticmethod
    def make_adapting_curve(name, pvals, do_adapt):
        if do_adapt in ['instant', 'instant-crime']:
            gamma_curve = FlatCurve(0)
        elif 'gamma' in ACRAController.adaptation[name]:
            gamma = ACRAController.adaptation[name]['gamma'].eval_pval(0, pvals[name + '_gamma'], 1e-2)
            if ACRAController.adaptation[name]['cut-point'] == 'killdd':
                gamma_curve = StepCurve([-40, 29, 80], [1, gamma])
            else:
                gamma_curve = StepCurve([-40, 29, 80], [gamma, gamma])
        else:
            yrmod_before = ACRAController.adaptation[name]['time'][0]
            yrmod_after = ACRAController.adaptation[name]['time'][1]
            beta_infinity = float(ACRAController.adaptation[name]['beta-infinity'])

            cut_point = float(ACRAController.adaptation[name]['cut-point'])
            (betas_before_left, betas_before_right) = AdaptingCurve.get_betas(ACRAController.make_curve(yrmod_before[1], pvals[name + '_time_' + str(yrmod_before[0])]), cut_point)
            (betas_after_left, betas_after_right) = AdaptingCurve.get_betas(ACRAController.make_curve(yrmod_after[1], pvals[name + '_time_' + str(yrmod_after[0])]), cut_point)

            gammas_left = AdaptingCurve.calculate_gammas(betas_before_left, yrmod_before[0], betas_after_left, yrmod_after[0], beta_infinity)
            gammas_right = AdaptingCurve.calculate_gammas(betas_before_right, yrmod_before[0], betas_after_right, yrmod_after[0], beta_infinity)

            gamma_curve = StepCurve([-40, cut_point, 80], [np.mean(gammas_left), np.mean(gammas_right)])

        if 'baseline_pval' in ACRAController.adaptation[name]:
            pval_baseline = pvals[ACRAController.adaptation[name]['baseline_pval']]
        else:
            pval_baseline = pvals[ACRAController.adaptation[name]['baseline']]

        curve_baseline = ACRAController.make_curve(ACRAController.models[ACRAController.adaptation[name]['baseline']], pval_baseline)
        xx = curve_baseline.get_xx()

        if do_adapt in ['complete', 'complete-crime']:
            curve_future = FlatCurve(beta_infinity)
            return SimpleAdaptingCurve(xx, curve_baseline, curve_future, gamma_curve)
        elif 'space' in ACRAController.adaptation[name]:
            curve_others = [ACRAController.make_curve(ACRAController.adaptation[name]['space'][key], pvals[name + '_space_' + str(key)]) for key in ACRAController.adaptation[name]['space']]
            Wbar_baseline = float(ACRAController.adaptation[name]['Wbar-baseline'])
            Wbar_others = map(float, ACRAController.adaptation[name]['space'].keys())

            return AdaptingCurve(xx, curve_baseline, curve_others, Wbar_baseline, Wbar_others, gamma_curve, clip_zero=ACRAController.adaptation[name]['clip-zero'])
        else:
            curve_future = ACRAController.make_curve(ACRAController.adaptation[name]['future'], pvals[name + '_future'])
            return SimpleAdaptingCurve(xx, curve_baseline, curve_future, gamma_curve)

    ### Helper Functions

    @staticmethod
    def make_curve(id, pval):
        print id
        if isinstance(id, UnivariateModel):
            model = id
        elif id[0:7] == 'http://':
            model = remote.view_model('url', id)
        elif '_' in id:
            return ACRAController.make_curve(ACRAController.models[id], pval)
        else:
            model = remote.view_model('model', id)

        if isinstance(model, BinModel):
            return StepCurve(model.get_xx(), [model.eval_pval(20, pval, 1e-2), model.eval_pval(35, pval, 1e-2)])

        model = MemoizedUnivariate(model)
        model.set_x_cache_decimals(1)

        return CurveCurve(model.get_xx(), model.get_eval_pval_spline(pval, (-40, 80), threshold=1e-2))

    @staticmethod
    def get_pval_names(do_adapt=False):
        names = ACRAController.models.keys()
        # Remove the crime adaptable: they use the same as non-adaptable
        names.remove('crime_violent_adaptable_tasmax_url')
        names.remove('crime_property_adaptable_tasmax_url')

        if do_adapt:
            for name in ACRAController.adaptation:
                if do_adapt not in ["complete", "complete-crime"]:
                    if 'space' in ACRAController.adaptation[name]:
                        names.extend([name + '_space_' + str(key) for key in ACRAController.adaptation[name]['space']])
                    else:
                        names.append(name + '_future')

                if do_adapt not in ["instant", "instant-crime"]:
                    if 'time' in ACRAController.adaptation[name]:
                        names.extend([name + '_time_' + str(yrmod[0]) for yrmod in ACRAController.adaptation[name]['time']])
                    if 'gamma' in ACRAController.adaptation[name]:
                        names.append(name + '_gamma')

        return names

    def profile(self):
        import cProfile, pstats, StringIO
        pr = cProfile.Profile()
        pr.enable()

        self.make_results(['testing'], lambda pdir: { name: random.random() for name in ACRAController.get_pval_names(False) }, only_one=True)

        pr.disable()
        s = StringIO.StringIO()
        sortby = 'cumulative'
        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)
        ps.print_stats()
        return s.getvalue()

    @staticmethod
    def load_acra_regions():
        regions = {}

        with open(aggregator.__path__[0] + "/lib/acra/regions/regionsANSI.csv") as countyfp:
            reader = csv.reader(countyfp, delimiter=',')
            reader.next() # skip header

            for row in reader:
                fips = row[0]
                if len(fips) < 5:
                    fips = '0' + fips

                regions[fips] = row[2]

        return regions

    ### Aggregation to Region Levels

    @staticmethod
    def aggregate_scales(scales, get_region, use_combo_lines=False):
        ## if get region is national, check for 00000
        if get_region('TT123') == 'national':
            if use_combo_lines and '00000' in scales: # Return the national, if we have it
                return dict(national=scales['00000'])
            else: # Sum all approved counties otherwise
                scale = 0
                for key in ACRAController.load_acra_regions().keys():
                    scale += scales.get(key, 0)
                return dict(national=scale)

        # if get region is state and not using combo, sum over approved counties
        if not use_combo_lines and get_region('TT123') == 'TT':
            newscales = {}
            for key in ACRAController.load_acra_regions().keys():
                region = get_region(key)
                if region is None:
                    continue

                if region not in newscales:
                    newscales[region] = 0

                newscales[region] += scales.get(key, 0)

            return newscales

        # Just do normal aggregation
        newscales = {}

        for key in scales:
            region = get_region(key)
            if region is None:
                continue

            if region not in newscales:
                newscales[region] = 0

            newscales[region] += scales[key]

        # If get_region is for states, check for ##000
        if use_combo_lines and get_region('TT123') == 'TT':
            for key in scales:
                region = get_region(key)
                if region + '000' in scales:
                    newscales[region + '000'] = scales[region + '000']

        return newscales

    @staticmethod
    def get_county_scales(impact):
        def callback(scales):
            global saved_scales
            saved_scales = scales

        global saved_scales
        saved_scales = None

        if impact == 'crime-violent':
            ACRAController.crime_aggregate_tar(None, 0, None, callback=callback)
        elif impact == 'crime-property':
            ACRAController.crime_aggregate_tar(None, 1, None, callback=callback)
        elif impact == 'health-mortality':
            ACRAController.population_aggregate_tar(None, None, callback=callback)
        elif impact in ['health-mortage-0-0', 'health-mortage-1-44', 'health-mortage-45-64', 'health-mortage-65-inf']:
            saved_scales = mortality.load_age_populations(impact[len('health-mortage-'):], census.get_populations_2010(True))
        elif impact == 'labor-high-productivity':
            ACRAController.labor_aggregate_tar(None, None, True, callback=callback)
        elif impact == 'labor-low-productivity':
            ACRAController.labor_aggregate_tar(None, None, False, callback=callback)
        elif impact == 'labor-total-productivity':
            ACRAController.labor_total_aggregate_tar(None, None, callback=callback)
        elif impact in ['yields-maize', 'yields-maize-noco2']:
            saved_scales = agriculture.aggregate_tar_with_scale_file(None, ['maize-prod2'], [1], return_it=True)
        elif impact in ['yields-wheat', 'yields-wheat-noco2']:
            saved_scales = agriculture.aggregate_tar_with_scale_file(None, ['wheat-prod2'], [1], return_it=True)
        elif impact in ['yields-cotton', 'yields-cotton-noco2']:
            saved_scales = agriculture.aggregate_tar_with_scale_file(None, ['cotton-prod2'], [1], return_it=True)
        elif impact in ['yields-oilcrop', 'yields-oilcrop-noco2']:
            saved_scales = agriculture.aggregate_tar_with_scale_file(None, ['soy-prod2'], [1], return_it=True)
        elif impact in ['yields-total', 'yields-total-noco2']:
            saved_scales = agriculture.aggregate_tar_with_scale_file(None, ['maize-prod2','wheat-prod2', 'cotton-prod2', 'soy-prod2'], [1] * 4, return_it=True)
        elif impact in ['yields-grains', 'yields-grains-noco2']:
            saved_scales = agriculture.aggregate_tar_with_scale_file(None, ['maize-prod2','wheat-prod2'], [1, 1], return_it=True) # Use MT here, for producing absolutes

        return saved_scales

    @staticmethod
    def callback_with_scales(make_callback):
        impacts = ['crime-violent', 'crime-property', 'health-mortality', 'health-mortage-0-0', 'health-mortage-1-44', 'health-mortage-45-64', 'health-mortage-65-inf', 'labor-high-productivity', 'labor-low-productivity', 'labor-total-productivity', 'yields-total', 'yields-total-noco2']
        for impact in impacts:
            make_callback(impact)(ACRAController.get_county_scales(impact))

    def scale_bycounty(self):
        default_func = lambda x: x - 1
        default_suffix = 'absolute'
        do_historical = False

        if do_historical:
            year0s = [0, -20, -30]
            rcps = ['truehist']
            do_noco2 = True
        else:
            year0s = [2020, 2040, 2080]
            rcps = ['rcp26', 'rcp45', 'rcp60', 'rcp85']
            do_noco2 = False

        root = "/home/jrising/aggregator/trunk/acra/extract/bycounty"
        get_region = None

        #root = "/home/jrising/aggregator/trunk/acra/extract/bystate"
        #get_region = lambda fips: fips[0:2]

        #root = "/home/jrising/aggregator/trunk/acra/extract/bynational"
        #get_region = lambda fips: 'national'

        #root = "/home/jrising/aggregator/trunk/acra/extract/byregion"
        #regions = ACRAController.load_acra_regions()
        #get_region = lambda fips: regions.get(fips, None)

        #root = "/home/jrising/aggregator/trunk/acra/extract/bynca"
        #regions = ACRAController.load_acra_regions()
        #get_region = lambda fips: regions.get(fips, None)
        #default_func = lambda x: x
        #default_suffix = 'scaled'
        #do_noco2 = False

        def make_callback(impact, func=default_func, mult=lambda s, xx: s * xx):
            print impact

            if impact[0:6] == 'health':
                func = lambda x: x

            def callback(scales, suffix=default_suffix):
                print len(scales)
                if get_region is not None:
                    scales = ACRAController.aggregate_scales(scales, get_region)

                for rcp in rcps:
                    for year0 in year0s:
                        with open(os.path.join(root, impact + '-' + rcp + '-' + str(year0) + '.csv')) as csvfp:
                            reader = csv.reader(csvfp)

                            with open(os.path.join(root, impact + '-' + rcp + '-' + str(year0) + '-' + suffix + '.csv'), 'w') as outfp:
                                writer = csv.writer(outfp, quoting=csv.QUOTE_MINIMAL)
                                writer.writerow(reader.next())

                                for row in reader:
                                    if row[0] not in scales:
                                        continue

                                    scaled = mult(scales[row[0]], np.array(map(func, map(float, row[1:]))))
                                    writer.writerow([row[0]] + list(scaled))

            return callback

        ACRAController.callback_with_scales(make_callback)

        if get_region is None:
            make_callback('health-mortality', lambda x: x, mult=lambda s, xx: (s + xx) / s)(mortality.load_mortality_rates(), suffix='fraction')

        return

        # Below generates yields-total from individuals (but now we generate directly!)

        if do_noco2:
            midfix = 'noco2-'
        else:
            midfix = ''

        scales1 = agriculture.aggregate_tar_with_scale_file(None, ['maize-prod2','wheat-prod2'], [1, 1], return_it=True)
        scales2 = agriculture.aggregate_tar_with_scale_file(None, ['cotton-prod2'], [1], return_it=True)
        scales3 = agriculture.aggregate_tar_with_scale_file(None, ['soy-prod2'], [1], return_it=True)
        if get_region is not None:
            scales1 = ACRAController.aggregate_scales(scales1, get_region)
            scales2 = ACRAController.aggregate_scales(scales2, get_region)
            scales3 = ACRAController.aggregate_scales(scales3, get_region)

        for rcp in ['rcp26', 'rcp45', 'rcp60', 'rcp85']:
            for year0 in [2020, 2040, 2080]:
                with open(os.path.join(root, 'yields-grains-' + midfix + rcp + '-' + str(year0) + '.csv')) as csvfp1:
                    with open(os.path.join(root, 'yields-cotton-' + midfix + rcp + '-' + str(year0) + '.csv')) as csvfp2:
                        with open(os.path.join(root, 'yields-oilcrop-' + midfix + rcp + '-' + str(year0) + '.csv')) as csvfp3:
                            reader1 = csv.reader(csvfp1)
                            reader2 = csv.reader(csvfp2)
                            reader3 = csv.reader(csvfp3)

                            with open(os.path.join(root, 'yields-total-' + midfix + rcp + '-' + str(year0) + '.csv'), 'w') as outfp1:
                                with open(os.path.join(root, 'yields-total-' + midfix + rcp + '-' + str(year0) + '-' + default_suffix + '.csv'), 'w') as outfp2:
                                    writer1 = csv.writer(outfp1, quoting=csv.QUOTE_MINIMAL)
                                    writer1.writerow(reader1.next())

                                    writer2 = csv.writer(outfp2, quoting=csv.QUOTE_MINIMAL)
                                    writer2.writerow(reader2.next())

                                    reader3.next()

                                    # put all of reader rows into dicts
                                    reader1rows = {}
                                    for row1 in reader1:
                                        rowlen = len(row1) - 1
                                        reader1rows[row1[0]] = np.array(map(default_func, map(float, row1[1:])))

                                    reader2rows = {}
                                    for row2 in reader2:
                                        reader2rows[row2[0]] = np.array(map(default_func, map(float, row2[1:])))

                                    reader3rows = {}
                                    for row3 in reader3:
                                        reader3rows[row3[0]] = np.array(map(default_func, map(float, row3[1:])))

                                    for fips in set(reader1rows.keys() + reader2rows.keys() + reader3rows.keys()):
                                        if scales1.get(fips, 0) + scales2.get(fips, 0) + scales3.get(fips, 0) == 0:
                                            continue
                                        print fips
                                        scaled = scales1.get(fips, 0) * reader1rows.get(fips, np.zeros(rowlen)) + scales2.get(fips, 0) * reader2rows.get(fips, np.zeros(rowlen)) + scales3.get(fips, 0) * reader3rows.get(fips, np.zeros(rowlen))

                                        writer2.writerow([fips] + list(scaled / 1000)) # 1000 MT
                                        if default_suffix == 'scaled':
                                            writer1.writerow([fips] + list(scaled / (scales1.get(fips, 0) + scales2.get(fips, 0) + scales3.get(fips, 0))))
                                        else:
                                            writer1.writerow([fips] + list(1 + scaled / (scales1.get(fips, 0) + scales2.get(fips, 0) + scales3.get(fips, 0))))

    def make_totalcalories(self):
        # Collect the ACRA region definitions for regional aggregation
        regions = ACRAController.load_acra_regions()
        get_region = lambda fips: regions[fips] # passed to aggregate_tar

        for (variables, scenario, model) in effect_bundle.find_ncdfs_onereal():
            if scenario != 'rcp26' or model != 'fgoals-g2':
                continue

            targetdir = "/home/jrising/diagnostics/formike"
            #os.mkdir(targetdir)

            co2scale = ACRAController.make_co2scale(0)
            pvals = [.5] * 15

            effect_bundle.make_tar_ncdf('yields-grains', variables, ['tas', 'tasmin', 'tasmax', 'pr'],
                                        ACRAController.make_grains_generator(pvals, co2scale, dont_divide=True), targetdir, collabel=['calories'])
            agriculture.aggregate_tar_with_scale_file('yields-grains', ['maize-planted','wheat-planted'], [1690.,1615.], targetdir, True)
            agriculture.aggregate_tar_with_scale_file('yields-grains', ['maize-planted','wheat-planted'], [1690.,1615.], targetdir, get_region)
            agriculture.aggregate_tar_with_scale_file('yields-grains', ['maize-planted','wheat-planted'], [1690.,1615.], targetdir)

    def make_ncaregions(self):
        root = "/home/jrising/aggregator/trunk/acra/extract/bynca"
        ncas = {'Great Plains': ['GPL', 'TXC'], 'Northeast': ['NEA', 'NAC'], 'Southeast': ['SEA', 'SAC', 'GLF'], 'Northwest': ['NWE', 'NPC'], 'Southwest': ['SWE', 'SPC'], 'Hawaii': ['HWI'], 'Midwest': ['MWE'], 'Alaska': ['ALK']}

        for filename in os.listdir(root):
            if filename[-11:] == '-scaled.csv':
                print filename
                fullpath = os.path.join(root, filename)

                scaled = {}
                with open(fullpath) as csvfp:
                    reader = csv.reader(csvfp)
                    header = reader.next()

                    for row in reader:
                        scaled[row[0]] = np.array(map(float, row[1:]))

                scalings = {}
                with open(fullpath[0:-11] + '.csv') as csvfp:
                    reader = csv.reader(csvfp)
                    reader.next()

                    for row in reader:
                        scalings[row[0]] = scaled[row[0]] / np.array(map(float, row[1:]))
                        #print scalings[row[0]]  # should be all identical

                with open(fullpath[0:-11] + '-nca.csv', 'w') as csvfp:
                    writer = csv.writer(csvfp)
                    writer.writerow(header)

                    for key in ncas:
                        numer = np.zeros(len(header)-1)
                        denom = 0

                        for cge in ncas[key]:
                            if cge in scaled:
                                numer += scaled[cge]
                                denom += scalings[cge][0]

                        writer.writerow([key] + list(numer / denom))

                with open(fullpath[0:-11] + '-nca-absolute.csv', 'w') as csvfp:
                    writer = csv.writer(csvfp)
                    writer.writerow(header)

                    for key in ncas:
                        numer = np.zeros(len(header)-1)

                        for cge in ncas[key]:
                            if cge in scaled:
                                numer += scaled[cge]

                        writer.writerow([key] + list(numer))

    def acptables(self):
        regions = ACRAController.load_acra_regions()

        def get_scale(impact, region):
            saved_scales = ACRAController.get_county_scales(impact)

            if region == 'national':
                scales = ACRAController.aggregate_scales(saved_scales, lambda fips: 'national')
            elif len(region) == 3:
                scales = ACRAController.aggregate_scales(saved_scales, lambda fips: regions.get(fips, None))
            elif len(region) == 2:
                scales = ACRAController.aggregate_scales(saved_scales, lambda fips: fips[0:2])
            else:
                scales = saved_scales

            return scales.get(region, 0)

        #acptable.make_tables(get_scale)
        #midwesttable.make_tables(get_scale)
        #weightstable.make_tables(get_scale)
        #californiatable.make_tables(get_scale)
        print os.getcwd()
        unweightedtable.make_tables(get_scale)

    def write_scales(self):
        get_region = None
        root = "/home/jrising/aggregator/trunk/acra/extract/bycounty"

        # Just print out scales
        def make_callback(impact):
            print impact
            def callback(scales):
                if get_region is not None:
                    scales = ACRAController.aggregate_scales(scales, get_region)

                with open(os.path.join(root, impact + '-baseline.csv'), 'w') as outfp:
                    writer = csv.writer(outfp, quoting=csv.QUOTE_MINIMAL)
                    writer.writerow(['code', 'weight'])

                    for key in scales:
                        writer.writerow([key, scales[key]])

            return callback

        ACRAController.callback_with_scales(make_callback)

    ### Diagnostics

    def test_total_agriculture(self):
        if not os.path.exists('testing'):
            os.mkdir('testing')

        # Collect the ACRA region definitions for regional aggregation
        regions = ACRAController.load_acra_regions()
        get_region = lambda fips: regions[fips] # passed to aggregate_tar

        (variables, scenario, fulmod) = effect_bundle.get_variables('001', 'rcp60', 'hadgem2-ao')
        co2col = ['rcp26', 'rcp45', 'rcp60', 'rcp85'].index(scenario) + 1
        pvals = { name: .5 for name in ACRAController.get_pval_names(False) }

        self.make_agriculture(variables, 'testing', pvals, co2col, get_region, only_do='total')

    def diagnostic_adapt_1(self):
        basedir = 'adapt-1'
        # Collect the ACRA region definitions for regional aggregation
        regions = ACRAController.load_acra_regions()
        get_region = lambda fips: regions[fips] # passed to aggregate_tar

        for (variables, realization, scenario, model) in effect_bundle.find_ncdfs_allreal():
            if 'tas' not in variables or 'pr' not in variables or 'tasmin' not in variables:
                continue

            pvals = { name: .5 for name in ACRAController.get_pval_names(True) }
            targetdir = os.path.join(basedir, scenario, model, realization)
            try:
                os.makedirs(targetdir)
                results.make_pval_file(targetdir, pvals)
            except Exception, ex:
                print ex
                continue

            print targetdir

            self.make_crime_alladapt(variables, targetdir, pvals, get_region)

    def diagnostic_adapt_mortality(self):
        make_generator_adapt = self.make_health_mortality_generator({ name: .5 for name in ACRAController.get_pval_names(True) }, True)
        make_generator_noadt = self.make_health_mortality_generator({ name: .5 for name in ACRAController.get_pval_names(True) }, False)

        times = []
        temps = []
        for year in range(2000, 2100):
            times += [year * 1000 + day for day in range(1, 366)]
            temps += [float((year - 2000) / 50.0 + 10 + 273.15) for day in range(365)]

        generator_adapt = make_generator_adapt("TEST", times, temps)
        generator_noadt = make_generator_noadt("TEST", times, temps)
        for values_adapt in generator_adapt:
            values_noadt  = generator_noadt.next()
            print values_noadt, values_adapt

    def diagnostic_s1_overlay(self):
        onlyfips = ['17161']#, '01049', '39159']
        pval = .5
        years = range(2000, 2101)
        columns = ['tas', 'pr', 'co2', 'yields-grains', 'yields-cotton', 'yields-oilcrop', 'crime-violent', 'crime-property', 'energy-residential', 'health-mortality', 'labor-high-productivity', 'labor-low-productivity']

        for (variables, scenario, model) in effect_bundle.find_ncdfs_onereal():
            if scenario != 'rcp60' or model != 'miroc-esm-chem':
                continue

            tasroot = Dataset(variables['tas'], 'r+', format='NETCDF4')
            prroot = Dataset(variables['pr'], 'r+', format='NETCDF4')
            co2col = ['rcp26', 'rcp45', 'rcp60', 'rcp85'].index(scenario) + 1
            results = self.results_for_weather(years, tasroot, variables['tasmin'], variables['tasmax'], prroot, pval, onlyfips, co2col, columns)

            for fips in onlyfips:
                with open("diagnostic-s1-" + fips + ".csv", 'wb') as csvfp:
                    writer = csv.writer(csvfp, quoting=csv.QUOTE_MINIMAL)
                    writer.writerow(['year'] + columns)

                    for ii in range(len(years)):
                        writer.writerow([years[ii]] + list(results[fips][ii,]))

    def results_for_weather(self, years, tasroot, tasminroot, tasmaxroot, prroot, pval, onlyfips, co2col, columns):
        results = {fips: np.zeros((len(years), len(columns))) for fips in onlyfips}

        def targetfunc(category, fips, generator):
            if fips in onlyfips:
                for values in generator:
                    if values[0] > np.max(years):
                        break
                    results[fips][values[0] - 2000, columns.index(category)] = values[len(values)-1]

        if isinstance(tasroot, dict):
            tasrootorig = tasroot['original']
            tasrootdata = tasroot['data']
        else:
            tasrootorig = tasroot
            tasrootdata = tasroot.variables['tas']

        for fips in onlyfips:
            fipsindex = list(tasrootorig.variables['fips']).index(int(fips))
            for (year, temps) in weather.yearly_daily_ncdf(tasrootorig.variables['time'], tasrootdata[:,fipsindex]):
                if year > np.max(years):
                    break
                if len(temps) > 0:
                    results[fips][year - 2000, 0] = np.mean(temps - 273.15)

        if isinstance(prroot, dict):
            prrootorig = prroot['original']
            prrootdata = prroot['data']
        else:
            prrootorig = prroot
            prrootdata = prroot.variables['pr']

        for fips in onlyfips:
            fipsindex = list(prrootorig.variables['fips']).index(int(fips))
            for (year, precip) in weather.yearly_daily_ncdf(prrootorig.variables['time'], prrootdata[:,fipsindex]):
                if year > np.max(years):
                    break
                if len(precip) > 0:
                    results[fips][year - 2000, 1] = np.mean(precip)

        co2scale = ACRAController.make_co2scale(co2col)
        for year in years:
            for fips in onlyfips:
                results[fips][year - 2000, 2] = co2scale(1, 100, year)

        self.make_agriculture(dict(tas=tasroot, tasmin=tasminroot, tasmax=tasmaxroot, pr=prroot), targetfunc, [pval] * 15, co2col)
        self.make_crime(dict(tas=tasroot, pr=prroot), targetfunc, [pval] * 4)
        self.make_energy(tasroot, targetfunc, pval)
        self.make_health(tasroot, targetfunc, pval)
        self.make_labor(tasmaxroot, targetfunc, [pval] * 2)

        return results

    def diagnostic_s456_experiments(self):
        fips = '17161'
        pval = .5
        years = range(2000, 2011)
        columns = ['tas', 'pr', 'co2', 'yields-grains', 'yields-cotton', 'yields-oilcrop', 'crime-violent', 'crime-property', 'energy-residential', 'health-mortality', 'labor-high-productivity', 'labor-low-productivity']

        for (variables, scenario, model) in effect_bundle.find_ncdfs_onereal():
            if scenario != 'rcp60' or model != 'miroc-esm-chem':
                continue

            for experiment in ['baseline', 'warmer', 'wetter']:
                tasroot = Dataset(variables['tas'], 'r+', format='NETCDF4')
                tasmaxroot = Dataset(variables['tasmax'], 'r+', format='NETCDF4')
                tasminroot = Dataset(variables['tasmin'], 'r+', format='NETCDF4')
                prroot = Dataset(variables['pr'], 'r+', format='NETCDF4')

                if experiment == 'warmer':
                    tasroot = dict(original=tasroot, data=tasroot.variables['tas'][:,:] + 1)
                    tasmaxroot = dict(original=tasmaxroot, data=tasmaxroot.variables['tasmax'][:,:] + 1)
                    tasminroot = dict(original=tasminroot, data=tasminroot.variables['tasmin'][:,:] + 1)
                elif experiment == 'wetter':
                    prroot = dict(original=prroot, data=prroot.variables['pr'][:,:] * 1.3)

                results = self.results_for_weather(years, tasroot, tasminroot, tasmaxroot, prroot, pval, [fips], 0, columns)

                with open("diagnostic-s456-" + experiment + ".csv", 'wb') as csvfp:
                    writer = csv.writer(csvfp, quoting=csv.QUOTE_MINIMAL)
                    writer.writerow(['year'] + columns)

                    for ii in range(len(years)):
                        writer.writerow([years[ii]] + list(results[fips][ii,]))

    def diagnostic_s2_response_byp(self):
        psamples = [.1, .3, .5, .7, .9]
        (yyyyddd, tas, byyear) = fake_weather.make_sampling_365years(2000, -10, 50, 40)
        tas = np.array(tas) + 273.15
        tasmin = tas - 2
        tasmax = tas + 2
        (yyyyddd, pr) = fake_weather.make_constant_365years(2000, 1.0, 40)

        for cat in ['grain-east', 'grain-west', 'cotton', 'oilcrop-east', 'oilcrop-west', 'violent', 'property', 'mortality', 'high', 'low']:
            print cat
            results = [[] for ii in range(40)]

            for pval in psamples:
                print pval
                co2scale = ACRAController.make_co2scale(0)

                dailys = {'tas': tas, 'pr': pr, 'tasmin': tasmin, 'tasmax': tasmax}
                if cat == 'grain-east' or cat == 'grain-west':
                    make_generator = ACRAController.make_grains_generator([pval] * 15, co2scale)
                elif cat == 'cotton':
                    make_generator = ACRAController.make_cotton_generator([pval] * 15, co2scale)
                elif cat == 'oilcrop-east' or cat == 'oilcrop-west':
                    make_generator = ACRAController.make_oilcrop_generator([pval] * 15, co2scale)
                elif cat == 'violent':
                    make_generator = self.make_violent_crime_generator([pval] * 4)
                elif cat == 'property':
                    make_generator = self.make_property_crime_generator([pval] * 4)
                elif cat == 'mortality':
                    make_generator = self.make_health_mortality_generator(pval)
                elif cat == 'high':
                    make_generator = self.make_labor_high_generator([pval] * 2)
                elif cat == 'low':
                    make_generator = self.make_labor_low_generator([pval] * 2)

                if cat in ['grain-east', 'cotton', 'oilcrop-east']:
                    for (year, value) in make_generator('17161', yyyyddd, dailys, 0, -90):
                        results[year - 2000].append(value)
                elif cat in ['grain-west', 'oilcrop-west']:
                    for (year, value) in make_generator('17161', yyyyddd, dailys, 0, -110):
                        results[year - 2000].append(value)
                elif cat == 'mortality' or cat == 'part-violent':
                    for (year, value) in make_generator('17161', yyyyddd, dailys['tas']):
                        results[year - 2000].append(value)
                elif cat in ['high', 'low']:
                    for (year, value) in make_generator('17161', yyyyddd, dailys['tasmax']):
                        results[year - 2000].append(value)
                else:
                    for (year, value) in make_generator('17161', yyyyddd, dailys):
                        results[year - 2000].append(value)

            with open("diagnostic-s2-" + cat + "-temps.csv", 'wb') as csvfp:
                writer = csv.writer(csvfp, quoting=csv.QUOTE_MINIMAL)
                writer.writerow(['tas'] + psamples)

                for ii in range(len(results)):
                    if len(results[ii]) > 0:
                        writer.writerow([byyear[ii]] + results[ii])

    def diagnostic_input_distribution(self):
        fips = '01049'
        var = 'tas'
        years = [2012, 2080]
        rcp_only = 'rcp85'
        model_only = 'ccsm4'

        results = np.zeros((365, len(years)))

        for (variables, scenario, model) in effect_bundle.find_ncdfs_onereal():
            if scenario != rcp_only or model != model_only:
                continue

            tasroot = Dataset(variables[var], 'r+', format='NETCDF4')
            fipsindex = list(tasroot.variables['fips']).index(int(fips))
            for (year, temps) in weather.yearly_daily_ncdf(tasroot.variables['time'], tasroot.variables[var][:,fipsindex]):
                if year not in years:
                    continue
                if len(temps) > 0:
                    results[:,years.index(year)] = temps - 273.15

            with open("diagnostic-input-" + var + "-" + fips + ".csv", 'wb') as csvfp:
                writer = csv.writer(csvfp, quoting=csv.QUOTE_MINIMAL)
                writer.writerow(years)

                for ii in range(365):
                    writer.writerow(list(results[ii,]))

    def diagnostic_calculation(self):
        fips_only = '01049'
        var = 'tas'
        years = [2012, 2080]
        rcp_only = 'rcp85'
        model_only = 'ccsm4'

        model = ACRAController.models['mortality_1_44_tas_model']
        bounds = '1-44'
        make_generator = effect_bundle.make_instabase(
            effect_bundle.make_scale(daily.make_daily_yearlydaybins(model, pval=.5), mortality.load_mortality_age_rates(bounds)), 2012, lambda x, y: x - y)

        for (variables, scenario, model) in effect_bundle.find_ncdfs_onereal():
            if scenario != rcp_only or model != model_only:
                continue

            def fips_filter(name, fips, generator):
                if fips != fips_only:
                    return

                for values in generator:
                    print values

            effect_bundle.call_with_generator('diagnostic', variables[var], var, make_generator, fips_filter)

    def diagnostic_a2_degdays(self):
        for (variables, scenario, model) in effect_bundle.find_ncdfs_onereal():
            if scenario != 'rcp60' or model != 'miroc-esm-chem':
                continue

            tasroot = Dataset(variables['tas'], 'r+', format='NETCDF4')
            tasminroot = Dataset(variables['tasmin'], 'r+', format='NETCDF4')
            tasmaxroot = Dataset(variables['tasmax'], 'r+', format='NETCDF4')
            tasminvals = tasminroot.variables['tasmin'][:,:]
            tasmaxvals = tasmaxroot.variables['tasmax'][:,:]

            years = range(2000, 2010) + [2020, 2080]
            numcounty = len(tasroot.variables['fips'])
            dd_lowup = np.zeros((numcounty, len(years)))
            dd_above = np.zeros((numcounty, len(years)))

            for fipsindex in range(numcounty):
                mins = weather.yearly_daily_ncdf(tasminroot.variables['time'], tasminvals[:,fipsindex])
                maxs = weather.yearly_daily_ncdf(tasmaxroot.variables['time'], tasmaxvals[:,fipsindex])
                for (year, temps_mins) in mins:
                    (year, temps_maxs) = next(maxs)
                    if year not in years:
                        continue
                    if len(temps_mins) > 0:
                        print fipsindex, years.index(year)
                        dd_lowup[fipsindex, years.index(year)] = agriculture.above_threshold(temps_mins - 273.15, temps_maxs - 273.15, 10)
                        dd_above[fipsindex, years.index(year)] = agriculture.above_threshold(temps_mins - 273.15, temps_maxs - 273.15, 29)

            with open("diagnostic-a2-lowup.csv", 'wb') as csvfp:
                writer = csv.writer(csvfp, quoting=csv.QUOTE_MINIMAL)
                writer.writerow(years)

                for ii in range(numcounty):
                    writer.writerow(list(dd_lowup[ii,]))

            with open("diagnostic-a2-above.csv", 'wb') as csvfp:
                writer = csv.writer(csvfp, quoting=csv.QUOTE_MINIMAL)
                writer.writerow(years)

                for ii in range(numcounty):
                    writer.writerow(list(dd_above[ii,]))

    def diagnostic_a3_2040(self):
        filename = 'yields-oilcrop-national.tar.gz'
        pdir = 'pmed'
        for rcp in os.listdir(pdir):
            if rcp != 'rcp60':
                continue

            for model in os.listdir(os.path.join(pdir, rcp)):
                for realization in os.listdir(os.path.join(pdir, rcp, model)):
                    if realization != '001':
                        continue

                    targetdir = os.path.join(pdir, rcp, model, realization)

                    files = os.listdir(targetdir)
                    if filename in files:
                        with tarfile.open(os.path.join(targetdir, filename)) as tar:
                            fp = tar.extractfile('yields-oilcrop-national/national.csv')

                            total = 0
                            for line in fp:
                                row = line.split(',')
                                if row[0][0] == '2':
                                    if float(row[0]) >= 2040 and float(row[0]) < 2050:
                                        total += float(row[1])

                            print targetdir + "\t" + str(total / 10)

    def diagnostic_a4_2040(self):
        for (variables, scenario, model) in effect_bundle.find_ncdfs_onereal():
            if scenario != 'rcp60':
                continue

            tasroot = Dataset(variables['tasmax'], 'r+', format='NETCDF4')
            tas = tasroot.variables['tasmax'][:,:]

            total = 0
            for day in range(0, 10*365):
                total += np.mean(tas[day,:])

            temp_2000 = total / (10*365)

            total = 0
            for day in range(40*365, 50*365):
                total += np.mean(tas[day,:])

            temp_2040 = total / (10*365)

            print model + "\t" + str(temp_2040 - temp_2000)

    def diagnostic_a5_had60(self):
        # Collect the ACRA region definitions for regional aggregation
        regions = ACRAController.load_acra_regions()
        get_region = lambda fips: regions[fips] # passed to aggregate_tar

        co2scale = ACRAController.make_co2scale(0)
        pvals = [.5] * 15

        for (variables, scenario, model) in effect_bundle.find_ncdfs_onereal():
            if scenario != 'rcp60' and model != 'hadgem-es':
                continue

            os.mkdir('had60')
            targetdir = 'had60'
            effect_bundle.make_tar_ncdf('yields-maize', variables, ['tas', 'tasmin', 'tasmax', 'pr'],
                                        effect_bundle.make_instabase(effect_bundle.make_runaverage(ACRAController.make_maize_generator(pvals, co2scale), [1, 1, 1], [0.21, 0.28, 0.51]), 2012), targetdir, collabel=['fraction', 'output'])
            agriculture.aggregate_tar_with_scale_file('yields-maize', ['maize-planted'], [1], targetdir, True)
            agriculture.aggregate_tar_with_scale_file('yields-maize', ['maize-planted'], [1], targetdir)

    def diagnostic_a6_tempdays(self):
        for (variables, scenario, model) in effect_bundle.find_ncdfs_onereal():
            if scenario != 'rcp85' or model != 'hadgem2-ao':
                continue

            #os.mkdir('had85')
            targetdir = 'had85'

            effect_bundle.make_tar_ncdf('temps', variables['tas'], 'tas',
                                        daily.make_daily_percentwithin((np.array([-np.inf] + range(10, 100, 10) + [np.inf]) - 32.0) / 1.8), targetdir)
            effect_bundle.aggregate_tar('temps', None, targetdir, get_region=True, report_all=True)

    def diagnostic_a7_wolframtable(self):
        pvals = [.5] * 15
        targetdir = 'had85'

        for (variables, scenario, model) in effect_bundle.find_ncdfs_onereal():
            if scenario != 'rcp85' or model != 'hadgem2-ao':
                continue

            for tdiff in [0, 3, 5]:
                suffix = '-' + str(tdiff)

                tasroot = Dataset(variables['tas'], 'r+', format='NETCDF4')
                tasmaxroot = Dataset(variables['tasmax'], 'r+', format='NETCDF4')
                tasminroot = Dataset(variables['tasmin'], 'r+', format='NETCDF4')

                tasroot = dict(original=tasroot, data=tasroot.variables['tas'][:,:] + tdiff)
                tasmaxroot = dict(original=tasmaxroot, data=tasmaxroot.variables['tasmax'][:,:] + tdiff)
                tasminroot = dict(original=tasminroot, data=tasminroot.variables['tasmin'][:,:] + tdiff)

                ncdfs = {'tas': tasroot, 'tasmax': tasmaxroot, 'tasmin': tasminroot, 'pr': variables['pr']}

                co2scale = ACRAController.make_co2scale(0)

                effect_bundle.make_tar_ncdf('yields-maize' + suffix, ncdfs, ['tasmin', 'tasmax', 'pr'],
                                            effect_bundle.make_runaverage(ACRAController.make_maize_generator(pvals, co2scale), [1, 1, 1], [0.21, 0.28, 0.51]), targetdir, collabel=['fraction', 'output'])
                effect_bundle.make_tar_ncdf('yields-cotton' + suffix, ncdfs, ['tasmin', 'tasmax', 'pr'],
                                            effect_bundle.make_runaverage(ACRAController.make_cotton_generator(pvals, co2scale), [1, 1, 1], [0.21, 0.28, 0.51]), targetdir, collabel=['fraction', 'output'])
                effect_bundle.make_tar_ncdf('yields-oilcrop' + suffix, ncdfs, ['tasmin', 'tasmax', 'pr'],
                                            effect_bundle.make_runaverage(ACRAController.make_oilcrop_generator(pvals, co2scale), [1, 1, 1, 1], [0.1, 0.19, 0.52, 0.2]), targetdir, collabel=['fraction', 'output'])

                agriculture.aggregate_tar_with_scale_file('yields-maize' + suffix, ['maize-planted'], [1], targetdir, True)
                agriculture.aggregate_tar_with_scale_file('yields-cotton' + suffix, ['cotton-planted'], [1], targetdir, True)
                agriculture.aggregate_tar_with_scale_file('yields-oilcrop' + suffix, ['soy-planted'], [1], targetdir, True)

    def diagnostic_j1_doseresp(self):
        ACRAController.sample_model('diagnostic_j1_wheat_tas', ACRAController.models['wheat_tas_model'], np.linspace(10, 45, 40), flip=True)
        ACRAController.sample_co2_model('diagnostic_j1_wheat_co2', ACRAController.models['wheat_co2_model'], flip=True)
        ACRAController.sample_model('diagnostic_j1_maize_east_tas', ACRAController.models['maize_east_tas_model'], np.linspace(10, 45, 40), flip=True)
        ACRAController.sample_model('diagnostic_j1_maize_west_tas', ACRAController.models['maize_west_tas_model'], np.linspace(10, 45, 40), flip=True)
        ACRAController.sample_model('diagnostic_j1_maize_east_pr', ACRAController.models['maize_east_pr_model'], np.linspace(0, 1.5, 40), flip=True)
        ACRAController.sample_model('diagnostic_j1_maize_west_pr', ACRAController.models['maize_west_pr_model'], np.linspace(0, 1.5, 40), flip=True)
        ACRAController.sample_co2_model('diagnostic_j1_maize_co2', ACRAController.models['maize_co2_model'], flip=True)
        ACRAController.sample_model('diagnostic_j1_cotton_tas', ACRAController.models['cotton_tas_model'], np.linspace(10, 45, 40), flip=True)
        ACRAController.sample_model('diagnostic_j1_cotton_pr', ACRAController.models['cotton_pr_model'], np.linspace(0, 1.5, 40), flip=True)
        ACRAController.sample_co2_model('diagnostic_j1_cotton_co2', ACRAController.models['cotton_co2_url'], flip=True)
        ACRAController.sample_model('diagnostic_j1_soy_east_tas', ACRAController.models['soy_east_tas_model'], np.linspace(10, 45, 40), flip=True)
        ACRAController.sample_model('diagnostic_j1_soy_west_tas', ACRAController.models['soy_west_tas_model'], np.linspace(10, 45, 40), flip=True)
        ACRAController.sample_model('diagnostic_j1_soy_east_pr', ACRAController.models['soy_east_pr_model'], np.linspace(0, 1.5, 40), flip=True)
        ACRAController.sample_model('diagnostic_j1_soy_west_pr', ACRAController.models['soy_west_pr_model'], np.linspace(0, 1.5, 40), flip=True)
        ACRAController.sample_co2_model('diagnostic_j1_soy_co2', ACRAController.models['soy_co2_model'], flip=True)
        ACRAController.sample_model('diagnostic_j1_mortality_tas', ACRAController.models['mortality_tas_url'], np.linspace(-25, 45, 70), smooth=True)
        ACRAController.sample_model('diagnostic_j1_mortality_0_0_tas', ACRAController.models['mortality_0_0_tas_model'], np.linspace(-25, 45, 70), smooth=True)
        ACRAController.sample_model('diagnostic_j1_mortality_1_44_tas', ACRAController.models['mortality_1_44_tas_model'], np.linspace(-25, 45, 70), smooth=True)
        ACRAController.sample_model('diagnostic_j1_mortality_45_64_tas', ACRAController.models['mortality_45_64_tas_model'], np.linspace(-25, 45, 70), smooth=True)
        ACRAController.sample_model('diagnostic_j1_mortality_65_inf_tas', ACRAController.models['mortality_65_inf_tas_model'], np.linspace(-25, 45, 70), smooth=True)
        ACRAController.sample_model('diagnostic_j1_labouur_high_tasmax', ACRAController.models['labor_high_tasmax_model'], np.linspace(-10, 50, 60), flip=True, smooth=True)
        ACRAController.sample_model('diagnostic_j1_labouur_low_tasmax', ACRAController.models['labor_low_tasmax_model'], np.linspace(-10, 50, 60), flip=True, smooth=True)
        ACRAController.sample_model('diagnostic_j1_crime_violent_tas', ACRAController.models['crime_violent_tas_url'], np.linspace(-10, 45, 60), smooth=True)
        ACRAController.sample_model('diagnostic_j1_crime_violent_pr', ACRAController.models['crime_violent_pr_url'], np.linspace(0, 40, 60), smooth=True)
        ACRAController.sample_model('diagnostic_j1_crime_property_tas', ACRAController.models['crime_property_tas_url'], np.linspace(-10, 45, 60), smooth=True)
        ACRAController.sample_model('diagnostic_j1_crime_property_pr', ACRAController.models['crime_property_pr_url'], np.linspace(0, 40, 60), smooth=True)

    @staticmethod
    def sample_model(csvname, id, xvals, flip=False, smooth=False):
        psamples = [.1, .3, .5, .7, .9]

        if id[0:7] == 'http://':
            model = remote.view_model('url', id)
        else:
            model = remote.view_model('model', id)

        if smooth:
            model = MemoizedUnivariate(model)
            model.set_x_cache_decimals(1)

        with open(csvname + '.csv', 'wb') as csvfp:
            writer = csv.writer(csvfp, quoting=csv.QUOTE_MINIMAL)
            writer.writerow(['x'] + psamples)

            if smooth:
                for xval in model.xx:
                    if flip:
                        writer.writerow([xval] + [model.get_eval_pval_spline(1 - pval, (-40, 80), threshold=1e-2)(xval) for pval in psamples])
                    else:
                        writer.writerow([xval] + [model.get_eval_pval_spline(pval, (-40, 80), threshold=1e-2)(xval) for pval in psamples])
            else:
                for xval in xvals:
                    if flip:
                        writer.writerow([xval] + [model.eval_pval(xval, 1 - pval, 1e-2) for pval in psamples])
                    else:
                        writer.writerow([xval] + [model.eval_pval(xval, pval, 1e-2) for pval in psamples])

    @staticmethod
    def sample_co2_model(csvname, id, co2col=2, flip=False):
        years = range(2000, 2100)
        psamples = [.1, .3, .5, .7, .9]

        if id[0:7] == 'http://':
            model = remote.view_model('url', id)
        else:
            model = remote.view_model('model', id)
        co2scale = ACRAController.make_co2scale(co2col)

        with open(csvname + '.csv', 'wb') as csvfp:
            writer = csv.writer(csvfp, quoting=csv.QUOTE_MINIMAL)
            writer.writerow(['year'] + psamples)

            for year in years:
                if flip:
                    writer.writerow([year] + [co2scale(1, model.eval_pval(None, 1 - pval, 1e-2), year) for pval in psamples])
                else:
                    writer.writerow([year] + [co2scale(1, model.eval_pval(None, pval, 1e-2), year) for pval in psamples])

    def diagnostic_allcounty_by12(self):
        pval = .5
        results = {} # {model-realization: {county: {cols: vals}}}
        currmodreal = None
        currrcp = None

        def targetfunc(category, fips, generator):
            yearsum_2020 = 0
            yearsum_2040 = 0
            yearsum_2080 = 0
            for (year, value) in generator:
                if year >= 2020 and year < 2040:
                    yearsum_2020 += value
                if year >= 2040 and year < 2060:
                    yearsum_2040 += value
                if year >= 2080 and year < 2100:
                    yearsum_2080 += value

            if fips not in results[currmodreal]:
                results[currmodreal][fips] = {}
            county = results[currmodreal][fips]

            county[currrcp + '-2020'] = yearsum_2020 / 20
            county[currrcp + '-2040'] = yearsum_2040 / 20
            county[currrcp + '-2080'] = yearsum_2080 / 20

        for (variables, realization, scenario, model) in effect_bundle.find_ncdfs_allreal():
            currrcp = scenario
            currmodreal = model + '-' + realization

            if currmodreal not in results:
                results[currmodreal] = {}

            self.make_health(variables['tas'], targetfunc, pval)

            if len(results[currmodreal]['17161']) == 12:
                columns = ['fips', 'rcp26-2020', 'rcp26-2040', 'rcp26-2080', 'rcp45-2020', 'rcp45-2040', 'rcp45-2080', 'rcp60-2020', 'rcp60-2040', 'rcp60-2080', 'rcp85-2020', 'rcp85-2040', 'rcp85-2080']
                with open("health-" + currmodreal + ".csv", 'wb') as csvfp:
                    writer = csv.writer(csvfp, quoting=csv.QUOTE_MINIMAL)
                    writer.writerow(columns)

                    for fips in results[currmodreal]:
                        county = results[currmodreal][fips]
                        writer.writerow([fips] + [county[name] for name in columns[1:]])

    @staticmethod
    def aggregate_scales(scales, get_region, use_combo_lines=False):
        ## if get region is national, check for 00000
        if get_region('TT123') == 'national':
            if use_combo_lines and '00000' in scales: # Return the national, if we have it
                return dict(national=scales['00000'])
            else: # Sum all approved counties otherwise
                scale = 0
                for key in ACRAController.load_acra_regions().keys():
                    scale += scales.get(key, 0)
                return dict(national=scale)

        # if get region is state and not using combo, sum over approved counties
        if not use_combo_lines and get_region('TT123') == 'TT':
            newscales = {}
            for key in ACRAController.load_acra_regions().keys():
                region = get_region(key)
                if region is None:
                    continue

                if region not in newscales:
                    newscales[region] = 0

                newscales[region] += scales.get(key, 0)

            return newscales

        # Just do normal aggregation
        newscales = {}

        for key in scales:
            region = get_region(key)
            if region is None:
                continue

            if region not in newscales:
                newscales[region] = 0

            newscales[region] += scales[key]

        # If get_region is for states, check for ##000
        if use_combo_lines and get_region('TT123') == 'TT':
            for key in scales:
                region = get_region(key)
                if region + '000' in scales:
                    newscales[region + '000'] = scales[region + '000']

        return newscales

    @staticmethod
    def get_county_scales(impact):
        def callback(scales):
            global saved_scales
            saved_scales = scales

        global saved_scales
        saved_scales = None

        if impact == 'crime-violent':
            callback(crime.load_crime_rates(0, census))
        elif impact == 'crime-property':
            callback(crime.load_crime_rates(1, census))
        elif impact == 'health-mortality':
            ACRAController.population_aggregate_tar(None, None, callback=callback)
        elif impact in ['health-mortage-0-0', 'health-mortage-1-44', 'health-mortage-45-64', 'health-mortage-65-inf']:
            saved_scales = mortality.load_age_populations(impact[len('health-mortage-'):], census.get_populations_2010(True))
        elif impact == 'labor-high-productivity':
            ACRAController.labor_aggregate_tar(None, None, True, callback=callback)
        elif impact == 'labor-low-productivity':
            ACRAController.labor_aggregate_tar(None, None, False, callback=callback)
        elif impact == 'labor-total-productivity':
            ACRAController.labor_total_aggregate_tar(None, None, callback=callback)
        elif impact in ['yields-maize', 'yields-maize-noco2']:
            saved_scales = agriculture.aggregate_tar_with_scale_file(None, ['maize-prod2'], [1], return_it=True)
        elif impact in ['yields-wheat', 'yields-wheat-noco2']:
            saved_scales = agriculture.aggregate_tar_with_scale_file(None, ['wheat-prod2'], [1], return_it=True)
        elif impact in ['yields-cotton', 'yields-cotton-noco2']:
            saved_scales = agriculture.aggregate_tar_with_scale_file(None, ['cotton-prod2'], [1], return_it=True)
        elif impact in ['yields-oilcrop', 'yields-oilcrop-noco2']:
            saved_scales = agriculture.aggregate_tar_with_scale_file(None, ['soy-prod2'], [1], return_it=True)
        elif impact in ['yields-total', 'yields-total-noco2']:
            saved_scales = agriculture.aggregate_tar_with_scale_file(None, ['maize-prod2','wheat-prod2', 'cotton-prod2', 'soy-prod2'], [1] * 4, return_it=True)
        elif impact in ['yields-grains', 'yields-grains-noco2']:
            saved_scales = agriculture.aggregate_tar_with_scale_file(None, ['maize-prod2','wheat-prod2'], [1, 1], return_it=True) # Use MT here, for producing absolutes

        return saved_scales

    @staticmethod
    def callback_with_scales(make_callback):
        impacts = ['crime-violent', 'crime-property', 'health-mortality', 'health-mortage-0-0', 'health-mortage-1-44', 'health-mortage-45-64', 'health-mortage-65-inf', 'labor-high-productivity', 'labor-low-productivity', 'labor-total-productivity', 'yields-total', 'yields-total-noco2']
        for impact in impacts:
            make_callback(impact)(ACRAController.get_county_scales(impact))

    def count_outofsamples(self):
        with open("outofsamples.csv", 'wb') as csvfp:
            writer = csv.writer(csvfp, quoting=csv.QUOTE_MINIMAL)
            writer.writerow(['scenario', 'model', 'avg-q05', 'avg-q50', 'avg-q95', 'max-q05', 'max-q50', 'max-q95'])

            for (variables, scenario, model) in effect_bundle.find_ncdfs_onereal():
                if scenario != 'rcp85':
                    continue

                if 'tas' not in variables or 'pr' not in variables or 'tasmin' not in variables:
                    continue

                rootgrp_max = Dataset(variables['tasmax'], 'r+', format='NETCDF4')
                rootgrp_avg = Dataset(variables['tas'], 'r+', format='NETCDF4')
                counties = rootgrp_max.variables['fips']

                tasmax = rootgrp_max.variables['tasmax'][:,:]
                tas = rootgrp_avg.variables['tas'][:,:]

                above_max = set()
                above_avg = set()
                for ii in range(len(counties)):
                    print counties[ii]
                    above_max.add(sum(tasmax[:,ii] > 40 + 273.15))
                    above_avg.add(sum(tas[:,ii] > 35 + 273.15))

                writer.writerow([scenario, model] + np.percentile(list(above_avg), [5, 50, 95]) + np.percentile(list(above_max), [5, 50, 95]))

    def diagnostic_days_by_bin(self):
        fips_only = '48215'
        model_tasmax = remote.view_model('url', ACRAController.models['crime_property_tasmax_url'])

        make_generator = fake.make_print_bymonthdaybins(model_tasmax)

        (variables, scenario, model) = effect_bundle.get_variables('001', 'rcp85', 'ccsm4')

        def iterate(name, fips, generator):
            if fips != fips_only:
                return

            for values in generator:
                print values

        effect_bundle.call_with_generator(None, variables['tasmax'], 'tasmax', make_generator, iterate)
